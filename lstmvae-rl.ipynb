{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11422967,"sourceType":"datasetVersion","datasetId":7153855},{"sourceId":11424466,"sourceType":"datasetVersion","datasetId":7154956}],"dockerImageVersionId":31011,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/vaasini889123/lstmvae-rl?scriptVersionId=239253711\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, LSTM, Dense, Dropout, RepeatVector, TimeDistributed, Masking, BatchNormalization, Bidirectional\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nimport gym\nfrom gym import spaces","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:02.787168Z","iopub.execute_input":"2025-04-16T17:57:02.787964Z","iopub.status.idle":"2025-04-16T17:57:16.467133Z","shell.execute_reply.started":"2025-04-16T17:57:02.787936Z","shell.execute_reply":"2025-04-16T17:57:16.466557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Lambda, TimeDistributed\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.468556Z","iopub.execute_input":"2025-04-16T17:57:16.469159Z","iopub.status.idle":"2025-04-16T17:57:16.473808Z","shell.execute_reply.started":"2025-04-16T17:57:16.469137Z","shell.execute_reply":"2025-04-16T17:57:16.473268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom tensorflow.keras.models import save_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.474548Z","iopub.execute_input":"2025-04-16T17:57:16.474864Z","iopub.status.idle":"2025-04-16T17:57:16.488603Z","shell.execute_reply.started":"2025-04-16T17:57:16.474845Z","shell.execute_reply":"2025-04-16T17:57:16.487908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, RepeatVector, Layer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.490303Z","iopub.execute_input":"2025-04-16T17:57:16.490547Z","iopub.status.idle":"2025-04-16T17:57:16.502512Z","shell.execute_reply.started":"2025-04-16T17:57:16.490529Z","shell.execute_reply":"2025-04-16T17:57:16.501822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\nfrom sklearn.preprocessing import OneHotEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.503339Z","iopub.execute_input":"2025-04-16T17:57:16.503985Z","iopub.status.idle":"2025-04-16T17:57:16.515951Z","shell.execute_reply.started":"2025-04-16T17:57:16.503966Z","shell.execute_reply":"2025-04-16T17:57:16.515229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.colab import files","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.516549Z","iopub.execute_input":"2025-04-16T17:57:16.516731Z","iopub.status.idle":"2025-04-16T17:57:16.564728Z","shell.execute_reply.started":"2025-04-16T17:57:16.516717Z","shell.execute_reply":"2025-04-16T17:57:16.564163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.565355Z","iopub.execute_input":"2025-04-16T17:57:16.565526Z","iopub.status.idle":"2025-04-16T17:57:16.568684Z","shell.execute_reply.started":"2025-04-16T17:57:16.565512Z","shell.execute_reply":"2025-04-16T17:57:16.568136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import defaultdict, deque","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.569511Z","iopub.execute_input":"2025-04-16T17:57:16.569789Z","iopub.status.idle":"2025-04-16T17:57:16.580245Z","shell.execute_reply.started":"2025-04-16T17:57:16.569764Z","shell.execute_reply":"2025-04-16T17:57:16.579636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.580982Z","iopub.execute_input":"2025-04-16T17:57:16.581156Z","iopub.status.idle":"2025-04-16T17:57:16.59283Z","shell.execute_reply.started":"2025-04-16T17:57:16.581142Z","shell.execute_reply":"2025-04-16T17:57:16.592239Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nimport pickle","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.594597Z","iopub.execute_input":"2025-04-16T17:57:16.594856Z","iopub.status.idle":"2025-04-16T17:57:16.604967Z","shell.execute_reply.started":"2025-04-16T17:57:16.594842Z","shell.execute_reply":"2025-04-16T17:57:16.604317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.605626Z","iopub.execute_input":"2025-04-16T17:57:16.605816Z","iopub.status.idle":"2025-04-16T17:57:16.616169Z","shell.execute_reply.started":"2025-04-16T17:57:16.605802Z","shell.execute_reply":"2025-04-16T17:57:16.615498Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.616883Z","iopub.execute_input":"2025-04-16T17:57:16.617261Z","iopub.status.idle":"2025-04-16T17:57:16.626842Z","shell.execute_reply.started":"2025-04-16T17:57:16.617235Z","shell.execute_reply":"2025-04-16T17:57:16.626244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, accuracy_score\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:16.627885Z","iopub.execute_input":"2025-04-16T17:57:16.628068Z","iopub.status.idle":"2025-04-16T17:57:16.720328Z","shell.execute_reply.started":"2025-04-16T17:57:16.628054Z","shell.execute_reply":"2025-04-16T17:57:16.719774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.models import load_model\nfrom tensorflow.keras.utils import register_keras_serializable","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:57:17.437812Z","iopub.execute_input":"2025-04-16T17:57:17.438548Z","iopub.status.idle":"2025-04-16T17:57:17.444121Z","shell.execute_reply.started":"2025-04-16T17:57:17.438519Z","shell.execute_reply":"2025-04-16T17:57:17.443575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/lstm-vae/user_day_sequences (3).csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:24:23.196618Z","iopub.execute_input":"2025-04-15T15:24:23.196913Z","iopub.status.idle":"2025-04-15T15:24:23.617767Z","shell.execute_reply.started":"2025-04-15T15:24:23.19689Z","shell.execute_reply":"2025-04-15T15:24:23.616536Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = data['activity_sequence']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:24:31.637152Z","iopub.execute_input":"2025-04-15T15:24:31.637694Z","iopub.status.idle":"2025-04-15T15:24:31.646709Z","shell.execute_reply.started":"2025-04-15T15:24:31.637671Z","shell.execute_reply":"2025-04-15T15:24:31.645939Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast\n\ndef convert_to_int_list(activity_sequence):\n    try:\n        # Attempt to evaluate the string as a list\n        int_list = ast.literal_eval(activity_sequence)\n\n        # Check if all elements are integers and convert\n        if all(isinstance(item, int) for item in int_list):\n          return int_list\n        else:\n          return [int(x) for x in int_list]\n    except (SyntaxError, ValueError):\n        # Handle cases where the string is not a valid list representation\n        print(f\"Invalid activity sequence: {activity_sequence}\")\n        return []  # or handle it differently, e.g., raise an exception\n\n# Apply the function to your data\n# data['activity_sequence'] = data['activity_sequence'].apply(convert_to_int_list) # Original line causing the error\ndata = data.apply(convert_to_int_list) # Corrected line to apply function to the Series directly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:24:39.955883Z","iopub.execute_input":"2025-04-15T15:24:39.956215Z","iopub.status.idle":"2025-04-15T15:24:47.034934Z","shell.execute_reply.started":"2025-04-15T15:24:39.956192Z","shell.execute_reply":"2025-04-15T15:24:47.03409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Shuffle DataFrame\ndf = data.sample(frac=1, random_state=42).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:24:50.83636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Parse strings to lists\nsequences = [seq for seq in df]\n\n# Get unique labels\nall_labels = set()\nfor seq in sequences:\n    all_labels.update(seq)\nlabels = sorted(list(all_labels))  # e.g., [0, 1, 2, 3, 5, 7, 8, 10, 11, 12]\nV = len(labels)  # Vocabulary size, e.g., 13","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:24:56.841261Z","iopub.execute_input":"2025-04-15T15:24:56.841657Z","iopub.status.idle":"2025-04-15T15:24:57.119531Z","shell.execute_reply.started":"2025-04-15T15:24:56.841628Z","shell.execute_reply":"2025-04-15T15:24:57.117788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = OneHotEncoder(categories=[labels], handle_unknown='ignore') \nmax_len = 86  #longest sequence\n\n# Converting sequences to one-hot encoded arrays\nX = []\nfor seq in sequences:\n    seq_array = np.array(seq).reshape(-1, 1)\n    one_hot = encoder.fit_transform(seq_array).toarray() \n    # Pad to max_len\n    padded = np.pad(one_hot, ((0, max_len - len(seq)), (0, 0)), mode='constant')  # Shape: (max_len, V)\n    X.append(padded)\nX = np.array(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:25:02.981086Z","iopub.execute_input":"2025-04-15T15:25:02.981437Z","iopub.status.idle":"2025-04-15T15:28:29.507933Z","shell.execute_reply.started":"2025-04-15T15:25:02.981413Z","shell.execute_reply":"2025-04-15T15:28:29.507121Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('X.npy', X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:28:58.045185Z","iopub.execute_input":"2025-04-15T15:28:58.045508Z","iopub.status.idle":"2025-04-15T15:29:00.155749Z","shell.execute_reply.started":"2025-04-15T15:28:58.045486Z","shell.execute_reply":"2025-04-15T15:29:00.154703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"LSTM VAE FOR FEATURE EXTRACTION","metadata":{}},{"cell_type":"code","source":"class SimplifiedLSTMVAE:\n    def __init__(self, vocab_size, max_len, hidden_size=32, latent_dim=8):\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n        self.hidden_size = hidden_size\n        self.latent_dim = latent_dim\n        self.model = self.build_model()\n\n    def sampling(self, args):\n        mu, log_var = args\n        epsilon = K.random_normal(shape=(K.shape(mu)[0], self.latent_dim))\n        return mu + K.exp(0.5 * log_var) * epsilon\n\n    def build_model(self):\n        # Encoder\n        inputs = Input(shape=(self.max_len, self.vocab_size))\n        h = LSTM(self.hidden_size)(inputs)\n        mu = Dense(self.latent_dim)(h)\n        log_var = Dense(self.latent_dim)(h)\n        z = Lambda(self.sampling)([mu, log_var])\n\n        # Decoder\n        z_repeated = RepeatVector(self.max_len)(z)\n        decoder_out = LSTM(self.hidden_size, return_sequences=True)(z_repeated)\n        outputs = TimeDistributed(Dense(self.vocab_size, activation='softmax'))(decoder_out)\n\n        # Model\n        vae = Model(inputs, outputs)\n\n        # Add VAE loss via a custom layer\n        class VAELossLayer(tf.keras.layers.Layer):\n            def __init__(self, max_len, **kwargs):\n                super(VAELossLayer, self).__init__(**kwargs)\n                self.max_len = max_len\n\n            def call(self, inputs):\n                x, x_decoded, mu, log_var = inputs\n                # Reconstruction loss\n                recon_loss = tf.reduce_mean(\n                    tf.keras.losses.categorical_crossentropy(x, x_decoded)\n                ) * self.max_len\n\n                # KL divergence\n                kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mu) - tf.exp(log_var))\n\n                # Add loss to the layer\n                self.add_loss(recon_loss + 0.1 * kl_loss)\n\n                # Return the decoded output\n                return x_decoded\n\n        # Create and apply the custom loss layer\n        loss_layer = VAELossLayer(self.max_len)\n        outputs = loss_layer([inputs, outputs, mu, log_var])\n\n        # Update the model\n        vae = Model(inputs, outputs)\n\n        return vae\n\n    def get_encoder(self):\n        encoder = Model(self.model.input, self.model.layers[3].output)\n        return encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:13:12.40224Z","iopub.execute_input":"2025-04-15T16:13:12.402777Z","iopub.status.idle":"2025-04-15T16:13:12.411387Z","shell.execute_reply.started":"2025-04-15T16:13:12.402748Z","shell.execute_reply":"2025-04-15T16:13:12.410766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = np.load('/kaggle/working/X.npy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:37:45.861358Z","iopub.execute_input":"2025-04-15T15:37:45.861985Z","iopub.status.idle":"2025-04-15T15:37:46.692838Z","shell.execute_reply.started":"2025-04-15T15:37:45.861961Z","shell.execute_reply":"2025-04-15T15:37:46.692076Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"V = 12\nmax_len = 86","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:38:30.643685Z","iopub.execute_input":"2025-04-15T15:38:30.644376Z","iopub.status.idle":"2025-04-15T15:38:30.647529Z","shell.execute_reply.started":"2025-04-15T15:38:30.644354Z","shell.execute_reply":"2025-04-15T15:38:30.646754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train\nvae = SimplifiedLSTMVAE(vocab_size=V, max_len=max_len)\nvae.model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))\nvae.model.fit(X, X, epochs=10, batch_size=128, shuffle=False, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:38:33.178458Z","iopub.execute_input":"2025-04-15T15:38:33.179269Z","iopub.status.idle":"2025-04-15T15:46:09.984221Z","shell.execute_reply.started":"2025-04-15T15:38:33.179242Z","shell.execute_reply":"2025-04-15T15:46:09.983557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_vae_model(vae, save_dir=\"/kaggle/working/vae_model\"):\n    \"\"\"Save all components of the VAE\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Save models\n    save_model(vae.model, os.path.join(save_dir, \"vae.h5\"))\n    save_model(vae.get_encoder(), os.path.join(save_dir, \"encoder.h5\"))\n\n    # Save config\n    config = {\n        'vocab_size': vae.vocab_size,\n        'max_len': vae.max_len,\n        'hidden_size': vae.hidden_size,\n        'latent_dim': vae.latent_dim\n    }\n    with open(os.path.join(save_dir, 'config.json'), 'w') as f:\n        json.dump(config, f)\n    \n    print(f\"Model saved to {save_dir}\")\n\nsave_vae_model(vae, \"my_vae_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:48:43.023046Z","iopub.execute_input":"2025-04-15T15:48:43.023285Z","iopub.status.idle":"2025-04-15T15:48:43.067901Z","shell.execute_reply.started":"2025-04-15T15:48:43.02327Z","shell.execute_reply":"2025-04-15T15:48:43.067335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vae.model.fit(X, X, epochs=5 , batch_size=128, shuffle=False, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:49:00.505552Z","iopub.execute_input":"2025-04-15T15:49:00.506198Z","iopub.status.idle":"2025-04-15T15:52:52.965481Z","shell.execute_reply.started":"2025-04-15T15:49:00.506173Z","shell.execute_reply":"2025-04-15T15:52:52.964918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_vae_model(vae, save_dir=\"/kaggle/working/vae_model\"):\n    \"\"\"Save all components of the VAE\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Save models\n    save_model(vae.model, os.path.join(save_dir, \"vae.h5\"))\n    save_model(vae.get_encoder(), os.path.join(save_dir, \"encoder.h5\"))\n\n    # Save config\n    config = {\n        'vocab_size': vae.vocab_size,\n        'max_len': vae.max_len,\n        'hidden_size': vae.hidden_size,\n        'latent_dim': vae.latent_dim\n    }\n    with open(os.path.join(save_dir, 'config.json'), 'w') as f:\n        json.dump(config, f)\n    \n    print(f\"Model saved to {save_dir}\")\n\nsave_vae_model(vae, \"my_vae_model_2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:52:55.944645Z","iopub.execute_input":"2025-04-15T15:52:55.945567Z","iopub.status.idle":"2025-04-15T15:52:55.991119Z","shell.execute_reply.started":"2025-04-15T15:52:55.945542Z","shell.execute_reply":"2025-04-15T15:52:55.990398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vae.model.fit(X, X, epochs=5 , batch_size=128, shuffle=False, verbose=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:53:07.80352Z","iopub.execute_input":"2025-04-15T15:53:07.80381Z","iopub.status.idle":"2025-04-15T15:56:57.261021Z","shell.execute_reply.started":"2025-04-15T15:53:07.803789Z","shell.execute_reply":"2025-04-15T15:56:57.260381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_vae_model(vae, save_dir=\"/kaggle/working/vae_model\"):\n    \"\"\"Save all components of the VAE\"\"\"\n    os.makedirs(save_dir, exist_ok=True)\n    \n    # Save models\n    save_model(vae.model, os.path.join(save_dir, \"vae.h5\"))\n    save_model(vae.get_encoder(), os.path.join(save_dir, \"encoder.h5\"))\n\n    # Save config\n    config = {\n        'vocab_size': vae.vocab_size,\n        'max_len': vae.max_len,\n        'hidden_size': vae.hidden_size,\n        'latent_dim': vae.latent_dim\n    }\n    with open(os.path.join(save_dir, 'config.json'), 'w') as f:\n        json.dump(config, f)\n    \n    print(f\"Model saved to {save_dir}\")\n\nsave_vae_model(vae, \"my_vae_model_3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T15:56:57.261981Z","iopub.execute_input":"2025-04-15T15:56:57.262226Z","iopub.status.idle":"2025-04-15T15:56:57.307549Z","shell.execute_reply.started":"2025-04-15T15:56:57.26221Z","shell.execute_reply":"2025-04-15T15:56:57.307016Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"k fld validation\n","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5)\nfold_metrics = []\n_, log_var, _ = vae.encoder.predict(X)\nfor train_idx, val_idx in kf.split(X):\n    X_val = X[val_idx]\n    # Get reconstructions for validation fold\n    recon = vae.model.predict(X_val)\n    fold_metrics.append({\n        'fold_mse': np.mean(np.square(X_val - recon)),\n        'fold_kl': -0.5 * np.mean(1 + log_var - np.square(mu) - np.exp(log_var))\n    })\n\nprint(\"Cross-validated metrics:\", np.mean(fold_metrics, axis=0))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-04-15T16:01:34.908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"reconstruction quality detection","metadata":{}},{"cell_type":"code","source":"vae, encoder, decoder = load_vae_model(\"my_saved_vae\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:07:57.496304Z","iopub.execute_input":"2025-04-15T16:07:57.496628Z","iopub.status.idle":"2025-04-15T16:07:57.502468Z","shell.execute_reply.started":"2025-04-15T16:07:57.496607Z","shell.execute_reply":"2025-04-15T16:07:57.501671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"V=12\nmax_len=86","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:13:47.902144Z","iopub.execute_input":"2025-04-15T16:13:47.902863Z","iopub.status.idle":"2025-04-15T16:13:47.906264Z","shell.execute_reply.started":"2025-04-15T16:13:47.902835Z","shell.execute_reply":"2025-04-15T16:13:47.905453Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vae = SimplifiedLSTMVAE(vocab_size=V, max_len=max_len)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:13:49.542885Z","iopub.execute_input":"2025-04-15T16:13:49.543187Z","iopub.status.idle":"2025-04-15T16:13:50.893682Z","shell.execute_reply.started":"2025-04-15T16:13:49.543165Z","shell.execute_reply":"2025-04-15T16:13:50.893136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"max_len=12\nvocab_size=86","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:22:51.906498Z","iopub.execute_input":"2025-04-15T16:22:51.907113Z","iopub.status.idle":"2025-04-15T16:22:51.910822Z","shell.execute_reply.started":"2025-04-15T16:22:51.907084Z","shell.execute_reply":"2025-04-15T16:22:51.90991Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimplifiedLSTMVAE:\n    def __init__(self, vocab_size, max_len, hidden_size=32, latent_dim=8):\n        self.vocab_size = vocab_size\n        self.max_len = max_len\n        self.hidden_size = hidden_size\n        self.latent_dim = latent_dim\n        self.model = self.build_model()\n\n    def sampling(self, args):\n        mu, log_var = args\n        epsilon = K.random_normal(shape=(K.shape(mu)[0], self.latent_dim))\n        return mu + K.exp(0.5 * log_var) * epsilon\n\n    def build_model(self):\n        # Encoder\n        inputs = Input(shape=(self.max_len, self.vocab_size))\n        h = LSTM(self.hidden_size)(inputs)\n        mu = Dense(self.latent_dim)(h)\n        log_var = Dense(self.latent_dim)(h)\n        z = Lambda(sampling)([mu, log_var])\n\n        # Decoder\n        z_repeated = RepeatVector(self.max_len)(z)\n        decoder_out = LSTM(self.hidden_size, return_sequences=True)(z_repeated)\n        outputs = TimeDistributed(Dense(self.vocab_size, activation='softmax'))(decoder_out)\n\n        # Model\n        vae = Model(inputs, outputs)\n\n        # Add VAE loss via a custom layer\n        class VAELossLayer(tf.keras.layers.Layer):\n            def __init__(self, max_len, **kwargs):\n                super(VAELossLayer, self).__init__(**kwargs)\n                self.max_len = max_len\n\n            def call(self, inputs):\n                x, x_decoded, mu, log_var = inputs\n                # Reconstruction loss\n                recon_loss = tf.reduce_mean(\n                    tf.keras.losses.categorical_crossentropy(x, x_decoded)\n                ) * self.max_len\n\n                # KL divergence\n                kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mu) - tf.exp(log_var))\n\n                # Add loss to the layer\n                self.add_loss(recon_loss + 0.1 * kl_loss)\n\n                # Return the decoded output\n                return x_decoded\n\n        # Create and apply the custom loss layer\n        loss_layer = VAELossLayer(self.max_len)\n        outputs = loss_layer([inputs, outputs, mu, log_var])\n\n        # Update the model\n        vae = Model(inputs, outputs)\n\n        return vae\n\n    def get_encoder(self):\n        encoder = Model(self.model.input, self.model.layers[3].output)\n        return encoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:37:17.904548Z","iopub.execute_input":"2025-04-15T16:37:17.90517Z","iopub.status.idle":"2025-04-15T16:37:17.914874Z","shell.execute_reply.started":"2025-04-15T16:37:17.905142Z","shell.execute_reply":"2025-04-15T16:37:17.91395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vae_instance = SimplifiedLSTMVAE(vocab_size=86, max_len=12)\nvae_model = vae_instance.model\nvae_model.save('my_vae_model_3/vae.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:37:37.563914Z","iopub.execute_input":"2025-04-15T16:37:37.564752Z","iopub.status.idle":"2025-04-15T16:37:37.681161Z","shell.execute_reply.started":"2025-04-15T16:37:37.564722Z","shell.execute_reply":"2025-04-15T16:37:37.680614Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = load_model(\n    'my_vae_model_3/encoder.h5',\n    custom_objects={'sampling': sampling}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:39:25.716743Z","iopub.execute_input":"2025-04-15T16:39:25.717406Z","iopub.status.idle":"2025-04-15T16:39:25.749359Z","shell.execute_reply.started":"2025-04-15T16:39:25.717381Z","shell.execute_reply":"2025-04-15T16:39:25.748748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = np.load('/kaggle/working/X.npy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:39:55.971245Z","iopub.execute_input":"2025-04-15T16:39:55.97202Z","iopub.status.idle":"2025-04-15T16:40:06.83773Z","shell.execute_reply.started":"2025-04-15T16:39:55.971993Z","shell.execute_reply":"2025-04-15T16:40:06.83708Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_input = tf.random.uniform((1, vae.max_len, vae.vocab_size))\nz = encoder.predict(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:41:16.52169Z","iopub.execute_input":"2025-04-15T16:41:16.522473Z","iopub.status.idle":"2025-04-15T16:41:46.559672Z","shell.execute_reply.started":"2025-04-15T16:41:16.522449Z","shell.execute_reply":"2025-04-15T16:41:46.559071Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"z.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:43:50.491037Z","iopub.execute_input":"2025-04-15T16:43:50.491331Z","iopub.status.idle":"2025-04-15T16:43:50.496381Z","shell.execute_reply.started":"2025-04-15T16:43:50.491298Z","shell.execute_reply":"2025-04-15T16:43:50.4956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:44:03.061968Z","iopub.execute_input":"2025-04-15T16:44:03.062566Z","iopub.status.idle":"2025-04-15T16:44:03.067562Z","shell.execute_reply.started":"2025-04-15T16:44:03.062541Z","shell.execute_reply":"2025-04-15T16:44:03.066759Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reconstructed = vae.model.predict(X)\nrecon_loss = tf.keras.losses.categorical_crossentropy(X, reconstructed)\nrecon_loss = tf.reduce_mean(recon_loss).numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:46:41.225634Z","iopub.execute_input":"2025-04-15T16:46:41.226431Z","iopub.status.idle":"2025-04-15T16:48:01.245544Z","shell.execute_reply.started":"2025-04-15T16:46:41.226405Z","shell.execute_reply":"2025-04-15T16:48:01.244658Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"recon_loss","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:50:48.585834Z","iopub.execute_input":"2025-04-15T16:50:48.58614Z","iopub.status.idle":"2025-04-15T16:50:48.59141Z","shell.execute_reply.started":"2025-04-15T16:50:48.58612Z","shell.execute_reply":"2025-04-15T16:50:48.590609Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"rl","metadata":{}},{"cell_type":"code","source":"mu_all = encoder.predict(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:52:51.282101Z","iopub.execute_input":"2025-04-15T16:52:51.282676Z","iopub.status.idle":"2025-04-15T16:53:21.123476Z","shell.execute_reply.started":"2025-04-15T16:52:51.282649Z","shell.execute_reply":"2025-04-15T16:53:21.122841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mu_all.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:53:21.124664Z","iopub.execute_input":"2025-04-15T16:53:21.124966Z","iopub.status.idle":"2025-04-15T16:53:21.130043Z","shell.execute_reply.started":"2025-04-15T16:53:21.124937Z","shell.execute_reply":"2025-04-15T16:53:21.129301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_latent = pd.DataFrame(index=range(X.shape[0]))\n\ndf_latent['latent_vector'] = list(mu_all)\n\n# Verify latent vectors\nprint(\"Latent vector shape:\", mu_all.shape)\nprint(\"Sample latent vector:\", df_latent['latent_vector'].iloc[0])\n\nmu_mean = np.mean(mu_all, axis=0)\nmu_std = np.std(mu_all, axis=0)\nprint(\"Mu mean:\", mu_mean)  \nprint(\"Mu std:\", mu_std)    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:54:29.662178Z","iopub.execute_input":"2025-04-15T16:54:29.663128Z","iopub.status.idle":"2025-04-15T16:54:29.791561Z","shell.execute_reply.started":"2025-04-15T16:54:29.663095Z","shell.execute_reply":"2025-04-15T16:54:29.79068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_latent.to_pickle('vae_latent_vectors.pkl')  \nprint(\"Saved DataFrame to 'vae_latent_vectors.pkl'\")\nnp.save('mu_all.npy', mu_all)\nprint(\"Saved latent vectors to 'mu_all.npy'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T16:54:35.681528Z","iopub.execute_input":"2025-04-15T16:54:35.681941Z","iopub.status.idle":"2025-04-15T16:54:37.595513Z","shell.execute_reply.started":"2025-04-15T16:54:35.681918Z","shell.execute_reply":"2025-04-15T16:54:37.594618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"REINFORCEMENT LEARNING","metadata":{}},{"cell_type":"code","source":"beh = pd.read_csv('/kaggle/input/reqd-data-lstmrl/beh.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:01:03.47313Z","iopub.execute_input":"2025-04-15T18:01:03.473777Z","iopub.status.idle":"2025-04-15T18:01:03.532121Z","shell.execute_reply.started":"2025-04-15T18:01:03.473751Z","shell.execute_reply":"2025-04-15T18:01:03.531522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"info = pd.read_csv('/kaggle/input/reqd-data-lstmrl/merged_df.csv')\ninfo = info.drop(columns={'Unnamed: 0'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:27:50.939823Z","iopub.execute_input":"2025-04-15T18:27:50.940436Z","iopub.status.idle":"2025-04-15T18:27:51.612536Z","shell.execute_reply.started":"2025-04-15T18:27:50.940404Z","shell.execute_reply":"2025-04-15T18:27:51.611741Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"roles = pd.read_csv('/kaggle/input/reqd-data-lstmrl/role_mappings_users.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:01:29.685327Z","iopub.execute_input":"2025-04-15T18:01:29.685914Z","iopub.status.idle":"2025-04-15T18:01:29.696474Z","shell.execute_reply.started":"2025-04-15T18:01:29.685888Z","shell.execute_reply":"2025-04-15T18:01:29.695722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"roles['role'] = (\n    roles['role_0'] * 1 + \n    roles['role_1'] * 2 + \n    roles['role_2'] * 4 + \n    roles['role_3'] * 8\n)\n\n# Create role lookup dictionary\nrole_lookup = dict(zip(roles['user'], roles['role']))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:02:22.983808Z","iopub.execute_input":"2025-04-15T18:02:22.984079Z","iopub.status.idle":"2025-04-15T18:02:22.990367Z","shell.execute_reply.started":"2025-04-15T18:02:22.984062Z","shell.execute_reply":"2025-04-15T18:02:22.989479Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mu_all.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:03:04.359695Z","iopub.execute_input":"2025-04-15T18:03:04.3603Z","iopub.status.idle":"2025-04-15T18:03:04.364907Z","shell.execute_reply.started":"2025-04-15T18:03:04.360275Z","shell.execute_reply":"2025-04-15T18:03:04.364137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"roles.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:03:14.592136Z","iopub.execute_input":"2025-04-15T18:03:14.592654Z","iopub.status.idle":"2025-04-15T18:03:14.597473Z","shell.execute_reply.started":"2025-04-15T18:03:14.592628Z","shell.execute_reply":"2025-04-15T18:03:14.596739Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_features = pd.read_csv('/kaggle/input/lstm-vae/user_day_sequences (3).csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:05:54.339637Z","iopub.execute_input":"2025-04-15T18:05:54.340501Z","iopub.status.idle":"2025-04-15T18:05:54.64931Z","shell.execute_reply.started":"2025-04-15T18:05:54.340475Z","shell.execute_reply":"2025-04-15T18:05:54.648626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:07:00.242091Z","iopub.execute_input":"2025-04-15T18:07:00.242589Z","iopub.status.idle":"2025-04-15T18:07:00.376856Z","shell.execute_reply.started":"2025-04-15T18:07:00.242555Z","shell.execute_reply":"2025-04-15T18:07:00.376141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:06:01.729737Z","iopub.execute_input":"2025-04-15T18:06:01.730495Z","iopub.status.idle":"2025-04-15T18:06:01.752566Z","shell.execute_reply.started":"2025-04-15T18:06:01.730468Z","shell.execute_reply":"2025-04-15T18:06:01.751975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"info.iloc[:,:22]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:11:22.458578Z","iopub.execute_input":"2025-04-15T18:11:22.45923Z","iopub.status.idle":"2025-04-15T18:11:22.624789Z","shell.execute_reply.started":"2025-04-15T18:11:22.459203Z","shell.execute_reply":"2025-04-15T18:11:22.623903Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_ids = info['user']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:13:15.321961Z","iopub.execute_input":"2025-04-15T18:13:15.322487Z","iopub.status.idle":"2025-04-15T18:13:15.326234Z","shell.execute_reply.started":"2025-04-15T18:13:15.322463Z","shell.execute_reply":"2025-04-15T18:13:15.325487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:13:32.621931Z","iopub.execute_input":"2025-04-15T18:13:32.622242Z","iopub.status.idle":"2025-04-15T18:13:32.628973Z","shell.execute_reply.started":"2025-04-15T18:13:32.62222Z","shell.execute_reply":"2025-04-15T18:13:32.628129Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a = info.iloc[:,22:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:29:52.573201Z","iopub.execute_input":"2025-04-15T18:29:52.573826Z","iopub.status.idle":"2025-04-15T18:29:52.57955Z","shell.execute_reply.started":"2025-04-15T18:29:52.573801Z","shell.execute_reply":"2025-04-15T18:29:52.578751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:29:54.197034Z","iopub.execute_input":"2025-04-15T18:29:54.197581Z","iopub.status.idle":"2025-04-15T18:29:54.205529Z","shell.execute_reply.started":"2025-04-15T18:29:54.197558Z","shell.execute_reply":"2025-04-15T18:29:54.204641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"info","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:15:18.68187Z","iopub.execute_input":"2025-04-15T18:15:18.682503Z","iopub.status.idle":"2025-04-15T18:15:18.810764Z","shell.execute_reply.started":"2025-04-15T18:15:18.682482Z","shell.execute_reply":"2025-04-15T18:15:18.809979Z"}},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"info","metadata":{}},{"cell_type":"code","source":"labels=[1,2,3,4,5,6,7,8,9,10,11,12]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:31:29.967018Z","iopub.execute_input":"2025-04-15T18:31:29.967936Z","iopub.status.idle":"2025-04-15T18:31:29.971735Z","shell.execute_reply.started":"2025-04-15T18:31:29.9679Z","shell.execute_reply":"2025-04-15T18:31:29.970986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for seq in a:\n    print(a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:32:19.380147Z","iopub.execute_input":"2025-04-15T18:32:19.380892Z","iopub.status.idle":"2025-04-15T18:32:19.38666Z","shell.execute_reply.started":"2025-04-15T18:32:19.380862Z","shell.execute_reply":"2025-04-15T18:32:19.385809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"type(a)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:33:51.667595Z","iopub.execute_input":"2025-04-15T18:33:51.668252Z","iopub.status.idle":"2025-04-15T18:33:51.673174Z","shell.execute_reply.started":"2025-04-15T18:33:51.668223Z","shell.execute_reply":"2025-04-15T18:33:51.672446Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_to_int_list(activity_sequence):\n    try:\n        # Attempt to evaluate the string as a list\n        int_list = ast.literal_eval(activity_sequence)\n\n        # Check if all elements are integers and convert\n        if all(isinstance(item, int) for item in int_list):\n          return int_list\n        else:\n          return [int(x) for x in int_list]\n    except (SyntaxError, ValueError):\n        # Handle cases where the string is not a valid list representation\n        print(f\"Invalid activity sequence: {activity_sequence}\")\n        return []  # or handle it differently, e.g., raise an exception\n\n# Apply the function to your data\na['aggregated_sequence'] = a['aggregated_sequence'].apply(convert_to_int_list) # Original line causing the error\n#data = data.apply(convert_to_int_list) # Corrected line to apply function to the Series directly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:34:22.622901Z","iopub.execute_input":"2025-04-15T18:34:22.623766Z","iopub.status.idle":"2025-04-15T18:34:27.248453Z","shell.execute_reply.started":"2025-04-15T18:34:22.623736Z","shell.execute_reply":"2025-04-15T18:34:27.247632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"m = a['aggregated_sequence']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:35:46.55762Z","iopub.execute_input":"2025-04-15T18:35:46.558275Z","iopub.status.idle":"2025-04-15T18:35:46.56183Z","shell.execute_reply.started":"2025-04-15T18:35:46.558248Z","shell.execute_reply":"2025-04-15T18:35:46.560985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = OneHotEncoder(categories=[labels], handle_unknown='ignore')  # Removed sparse=False\nmax_len = 86  # Set based on longest sequence or analysis\n\n# Convert sequences to one-hot encoded arrays\nX = []\nfor seq in m:\n    seq_array = np.array(seq).reshape(-1, 1)\n    one_hot = encoder.fit_transform(seq_array).toarray()  # Convert to dense array using toarray()\n    # Pad to max_len\n    padded = np.pad(one_hot, ((0, max_len - len(seq)), (0, 0)), mode='constant')  # Shape: (max_len, V)\n    X.append(padded)\nX = np.array(X)  # Shape: (num_sequences, max_len, V)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:35:58.770023Z","iopub.execute_input":"2025-04-15T18:35:58.770592Z","iopub.status.idle":"2025-04-15T18:38:55.472372Z","shell.execute_reply.started":"2025-04-15T18:35:58.770567Z","shell.execute_reply":"2025-04-15T18:38:55.471585Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:38:55.474291Z","iopub.execute_input":"2025-04-15T18:38:55.474616Z","iopub.status.idle":"2025-04-15T18:38:55.47959Z","shell.execute_reply.started":"2025-04-15T18:38:55.474587Z","shell.execute_reply":"2025-04-15T18:38:55.478853Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lstm_encoder = load_model(\n    'my_vae_model_3/encoder.h5',\n    custom_objects={'sampling': sampling}\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:39:52.763288Z","iopub.execute_input":"2025-04-15T18:39:52.763842Z","iopub.status.idle":"2025-04-15T18:39:52.798243Z","shell.execute_reply.started":"2025-04-15T18:39:52.763819Z","shell.execute_reply":"2025-04-15T18:39:52.797471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"m.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:41:33.972642Z","iopub.execute_input":"2025-04-15T18:41:33.97326Z","iopub.status.idle":"2025-04-15T18:41:33.977877Z","shell.execute_reply.started":"2025-04-15T18:41:33.973232Z","shell.execute_reply":"2025-04-15T18:41:33.977181Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tensor = tf.convert_to_tensor(X, dtype=tf.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:42:06.425254Z","iopub.execute_input":"2025-04-15T18:42:06.426051Z","iopub.status.idle":"2025-04-15T18:42:09.320741Z","shell.execute_reply.started":"2025-04-15T18:42:06.426022Z","shell.execute_reply":"2025-04-15T18:42:09.319946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"mu_all = lstm_encoder.predict(tensor)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:42:18.712091Z","iopub.execute_input":"2025-04-15T18:42:18.71295Z","iopub.status.idle":"2025-04-15T18:42:46.455944Z","shell.execute_reply.started":"2025-04-15T18:42:18.712923Z","shell.execute_reply":"2025-04-15T18:42:46.455257Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('mu_all.npy',mu_all)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:42:48.200224Z","iopub.execute_input":"2025-04-15T18:42:48.200972Z","iopub.status.idle":"2025-04-15T18:42:48.223256Z","shell.execute_reply.started":"2025-04-15T18:42:48.200946Z","shell.execute_reply":"2025-04-15T18:42:48.222682Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = np.column_stack([\n    info,                # Original features (excluding VAE error)\n    np.array([role_lookup[uid] for uid in user_ids]),  # Lookup role for each user-day\n    mu_all                 # VAE reconstruction error\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:42:55.658178Z","iopub.execute_input":"2025-04-15T18:42:55.658789Z","iopub.status.idle":"2025-04-15T18:42:56.193988Z","shell.execute_reply.started":"2025-04-15T18:42:55.658767Z","shell.execute_reply":"2025-04-15T18:42:56.19312Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:43:20.626469Z","iopub.execute_input":"2025-04-15T18:43:20.626978Z","iopub.status.idle":"2025-04-15T18:43:20.632566Z","shell.execute_reply.started":"2025-04-15T18:43:20.626943Z","shell.execute_reply":"2025-04-15T18:43:20.631755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('X_combined', X_combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:43:25.24857Z","iopub.execute_input":"2025-04-15T18:43:25.249205Z","iopub.status.idle":"2025-04-15T18:43:25.841429Z","shell.execute_reply.started":"2025-04-15T18:43:25.249179Z","shell.execute_reply":"2025-04-15T18:43:25.840599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[:,2:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:43:27.832364Z","iopub.execute_input":"2025-04-15T18:43:27.832657Z","iopub.status.idle":"2025-04-15T18:43:27.83851Z","shell.execute_reply.started":"2025-04-15T18:43:27.832637Z","shell.execute_reply":"2025-04-15T18:43:27.837806Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = X_combined[:,2:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:43:32.365467Z","iopub.execute_input":"2025-04-15T18:43:32.365786Z","iopub.status.idle":"2025-04-15T18:43:32.369527Z","shell.execute_reply.started":"2025-04-15T18:43:32.365764Z","shell.execute_reply":"2025-04-15T18:43:32.368742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[:,20:21]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:44:27.144811Z","iopub.execute_input":"2025-04-15T18:44:27.145349Z","iopub.status.idle":"2025-04-15T18:44:27.150541Z","shell.execute_reply.started":"2025-04-15T18:44:27.145329Z","shell.execute_reply":"2025-04-15T18:44:27.149852Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = np.delete(X_combined, 20, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:44:58.319071Z","iopub.execute_input":"2025-04-15T18:44:58.319392Z","iopub.status.idle":"2025-04-15T18:44:58.392531Z","shell.execute_reply.started":"2025-04-15T18:44:58.319369Z","shell.execute_reply":"2025-04-15T18:44:58.391949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[20]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:50:08.759659Z","iopub.execute_input":"2025-04-15T18:50:08.760428Z","iopub.status.idle":"2025-04-15T18:50:08.765253Z","shell.execute_reply.started":"2025-04-15T18:50:08.760399Z","shell.execute_reply":"2025-04-15T18:50:08.764575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:45:39.691252Z","iopub.execute_input":"2025-04-15T18:45:39.691974Z","iopub.status.idle":"2025-04-15T18:45:39.696501Z","shell.execute_reply.started":"2025-04-15T18:45:39.691948Z","shell.execute_reply":"2025-04-15T18:45:39.695885Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[:,:21]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:46:03.484545Z","iopub.execute_input":"2025-04-15T18:46:03.485189Z","iopub.status.idle":"2025-04-15T18:46:03.490142Z","shell.execute_reply.started":"2025-04-15T18:46:03.485165Z","shell.execute_reply":"2025-04-15T18:46:03.489372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[:,20:21]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:49:36.372976Z","iopub.execute_input":"2025-04-15T18:49:36.373292Z","iopub.status.idle":"2025-04-15T18:49:36.37942Z","shell.execute_reply.started":"2025-04-15T18:49:36.373267Z","shell.execute_reply":"2025-04-15T18:49:36.378638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:46:37.715618Z","iopub.execute_input":"2025-04-15T18:46:37.71625Z","iopub.status.idle":"2025-04-15T18:46:37.719904Z","shell.execute_reply.started":"2025-04-15T18:46:37.716226Z","shell.execute_reply":"2025-04-15T18:46:37.719089Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"scaler = StandardScaler()\nX_normalized = X_combined.copy()\nX_normalized[:, :21] = scaler.fit_transform(X_combined[:, :21])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:46:50.636419Z","iopub.execute_input":"2025-04-15T18:46:50.637006Z","iopub.status.idle":"2025-04-15T18:46:51.468878Z","shell.execute_reply.started":"2025-04-15T18:46:50.636981Z","shell.execute_reply":"2025-04-15T18:46:51.468259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_normalized","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:47:02.166013Z","iopub.execute_input":"2025-04-15T18:47:02.166316Z","iopub.status.idle":"2025-04-15T18:47:02.1715Z","shell.execute_reply.started":"2025-04-15T18:47:02.166288Z","shell.execute_reply":"2025-04-15T18:47:02.170845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('X_normalised', X_normalized)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:47:54.595903Z","iopub.execute_input":"2025-04-15T18:47:54.596838Z","iopub.status.idle":"2025-04-15T18:47:54.97854Z","shell.execute_reply.started":"2025-04-15T18:47:54.596812Z","shell.execute_reply":"2025-04-15T18:47:54.977691Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0][]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:51:29.442564Z","iopub.execute_input":"2025-04-15T18:51:29.442883Z","iopub.status.idle":"2025-04-15T18:51:29.448145Z","shell.execute_reply.started":"2025-04-15T18:51:29.44286Z","shell.execute_reply":"2025-04-15T18:51:29.447525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"aligning labels size","metadata":{}},{"cell_type":"code","source":"merged_df = pd.read_csv('/kaggle/input/reqd-data-lstmrl/merged_df.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:04:22.636634Z","iopub.execute_input":"2025-04-15T19:04:22.63695Z","iopub.status.idle":"2025-04-15T19:04:23.230243Z","shell.execute_reply.started":"2025-04-15T19:04:22.636929Z","shell.execute_reply":"2025-04-15T19:04:23.229374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:04:24.387603Z","iopub.execute_input":"2025-04-15T19:04:24.38824Z","iopub.status.idle":"2025-04-15T19:04:24.472542Z","shell.execute_reply.started":"2025-04-15T19:04:24.388215Z","shell.execute_reply":"2025-04-15T19:04:24.471662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_df = beh.melt(\n    id_vars=['user'], \n    var_name='date', \n    value_name='is_anomaly'\n)\n\n# Convert to datetime and sort\nlabels_df['date'] = pd.to_datetime(labels_df['date'])\nlabels_df = labels_df.sort_values(['user', 'date'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:08:48.600356Z","iopub.execute_input":"2025-04-15T19:08:48.601196Z","iopub.status.idle":"2025-04-15T19:08:48.733807Z","shell.execute_reply.started":"2025-04-15T19:08:48.601169Z","shell.execute_reply":"2025-04-15T19:08:48.733223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:08:58.174643Z","iopub.execute_input":"2025-04-15T19:08:58.175243Z","iopub.status.idle":"2025-04-15T19:08:58.18461Z","shell.execute_reply.started":"2025-04-15T19:08:58.175218Z","shell.execute_reply":"2025-04-15T19:08:58.183639Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df = merged_df.rename(columns = {'date_only':'date'})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:09:24.591433Z","iopub.execute_input":"2025-04-15T19:09:24.592092Z","iopub.status.idle":"2025-04-15T19:09:24.611413Z","shell.execute_reply.started":"2025-04-15T19:09:24.592068Z","shell.execute_reply":"2025-04-15T19:09:24.610837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df['date'] = pd.to_datetime(merged_df['date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:10:15.153299Z","iopub.execute_input":"2025-04-15T19:10:15.153965Z","iopub.status.idle":"2025-04-15T19:10:15.192571Z","shell.execute_reply.started":"2025-04-15T19:10:15.153937Z","shell.execute_reply":"2025-04-15T19:10:15.192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aligned_df = pd.merge(\n    merged_df,\n    labels_df,\n    on=['user', 'date'],\n    how='inner'  # Only keep rows present in both\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:10:20.732815Z","iopub.execute_input":"2025-04-15T19:10:20.733461Z","iopub.status.idle":"2025-04-15T19:10:20.9009Z","shell.execute_reply.started":"2025-04-15T19:10:20.733438Z","shell.execute_reply":"2025-04-15T19:10:20.900254Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aligned_df = aligned_df.drop(columns='Unnamed: 0')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:10:59.944246Z","iopub.execute_input":"2025-04-15T19:10:59.944873Z","iopub.status.idle":"2025-04-15T19:10:59.963599Z","shell.execute_reply.started":"2025-04-15T19:10:59.944848Z","shell.execute_reply":"2025-04-15T19:10:59.962822Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"is_anomaly = aligned_df[['is_anomaly']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:11:22.642446Z","iopub.execute_input":"2025-04-15T19:11:22.64337Z","iopub.status.idle":"2025-04-15T19:11:22.64708Z","shell.execute_reply.started":"2025-04-15T19:11:22.643342Z","shell.execute_reply":"2025-04-15T19:11:22.646427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"is_anomaly","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:11:28.064087Z","iopub.execute_input":"2025-04-15T19:11:28.064669Z","iopub.status.idle":"2025-04-15T19:11:28.070597Z","shell.execute_reply.started":"2025-04-15T19:11:28.064642Z","shell.execute_reply":"2025-04-15T19:11:28.0698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"is_anomaly = is_anomaly.to_numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:14:04.490488Z","iopub.execute_input":"2025-04-15T19:14:04.490893Z","iopub.status.idle":"2025-04-15T19:14:04.494607Z","shell.execute_reply.started":"2025-04-15T19:14:04.490871Z","shell.execute_reply":"2025-04-15T19:14:04.493602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('anomaly_labels.npy',is_anomaly)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:14:22.617363Z","iopub.execute_input":"2025-04-15T19:14:22.617945Z","iopub.status.idle":"2025-04-15T19:14:22.623949Z","shell.execute_reply.started":"2025-04-15T19:14:22.617921Z","shell.execute_reply":"2025-04-15T19:14:22.623298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0][20:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T18:54:33.244897Z","iopub.execute_input":"2025-04-15T18:54:33.24517Z","iopub.status.idle":"2025-04-15T18:54:33.250085Z","shell.execute_reply.started":"2025-04-15T18:54:33.245152Z","shell.execute_reply":"2025-04-15T18:54:33.249412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:41:12.694159Z","iopub.execute_input":"2025-04-15T19:41:12.694763Z","iopub.status.idle":"2025-04-15T19:41:12.699833Z","shell.execute_reply.started":"2025-04-15T19:41:12.694731Z","shell.execute_reply":"2025-04-15T19:41:12.698966Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"dqn","metadata":{}},{"cell_type":"code","source":"X_combined.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:16:51.693392Z","iopub.execute_input":"2025-04-15T19:16:51.693681Z","iopub.status.idle":"2025-04-15T19:16:51.698647Z","shell.execute_reply.started":"2025-04-15T19:16:51.693661Z","shell.execute_reply":"2025-04-15T19:16:51.697897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:43:33.551653Z","iopub.execute_input":"2025-04-15T19:43:33.552336Z","iopub.status.idle":"2025-04-15T19:43:33.556226Z","shell.execute_reply.started":"2025-04-15T19:43:33.552308Z","shell.execute_reply":"2025-04-15T19:43:33.555343Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class StateBuilder:\n    def __init__(self, window_size=7):\n        self.window_size = window_size\n        self.user_histories = defaultdict(lambda: deque(maxlen=window_size))\n        \n    def build_state(self, user_id, current_features):\n        \"\"\"Build state vector from 29-dimensional input features\n        \n        Args:\n            user_id: Unique user identifier\n            current_features: Array with:\n                - First 20: User-day features\n                - 21st: Role value\n                - Last 8: Encoded activity sequence\n        \"\"\"\n        # Convert to numpy array and validate\n        features = np.array(current_features, dtype=np.float32)\n        if len(features) != 29:\n            raise ValueError(f\"Expected 29 features, got {len(features)}\")\n        \n        # Initialize history if empty\n        if len(self.user_histories[user_id]) == 0:\n            self.user_histories[user_id].extend(\n                [np.zeros(20, dtype=np.float32)] * self.window_size\n            )\n        \n        # Update history with first 20 features\n        self.user_histories[user_id].append(features[:20])\n        \n        # Calculate temporal statistics\n        history = np.array(self.user_histories[user_id])\n        temporal_mean = history.mean(axis=0)\n        temporal_std = history.std(axis=0)\n        \n        # Build final state vector (20 + 1 + 20 + 20 + 8 = 69 dimensions)\n        state = np.concatenate([\n            features[:20],       # Current features (20)\n            [features[20]],      # Role value (1)\n            temporal_mean,       # Historical mean (20)\n            temporal_std,        # Historical std (20)\n            features[21:]        # Activity sequence (8)\n        ], dtype=np.float32)\n        \n        return state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:44:04.764119Z","iopub.execute_input":"2025-04-15T19:44:04.764633Z","iopub.status.idle":"2025-04-15T19:44:04.770886Z","shell.execute_reply.started":"2025-04-15T19:44:04.764609Z","shell.execute_reply":"2025-04-15T19:44:04.770109Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, state_dim=69, action_dim=2):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=100000)\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        \n        # Main and target networks\n        self.model = self._build_model()\n        self.target_model = self._build_model()\n        self.update_target_network()\n    \n    def _build_model(self):\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(128, activation='relu', input_dim=self.state_dim),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(64, activation='relu'),\n            tf.keras.layers.Dense(self.action_dim)\n        ])\n        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.001))\n        return model\n    \n    def update_target_network(self):\n        self.target_model.set_weights(self.model.get_weights())\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def act(self, state, epsilon=None):\n        if epsilon is None:\n            epsilon = self.epsilon\n        if np.random.rand() <= epsilon:\n            return random.randrange(self.action_dim)\n        q_values = self.model.predict(state[np.newaxis], verbose=0)\n        return np.argmax(q_values[0])\n    \n    def replay(self, batch_size):\n        if len(self.memory) < batch_size:\n            return\n        \n        minibatch = random.sample(self.memory, batch_size)\n        states = np.array([t[0] for t in minibatch])\n        actions = np.array([t[1] for t in minibatch])\n        rewards = np.array([t[2] for t in minibatch])\n        next_states = np.array([t[3] for t in minibatch])\n        dones = np.array([t[4] for t in minibatch])\n        \n        targets = self.model.predict(states, verbose=0)\n        next_q_values = self.target_model.predict(next_states, verbose=0)\n        \n        for i in range(batch_size):\n            targets[i][actions[i]] = rewards[i] if dones[i] else rewards[i] + self.gamma * np.amax(next_q_values[i])\n        \n        self.model.fit(states, targets, batch_size=batch_size, verbose=0)\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:44:25.732497Z","iopub.execute_input":"2025-04-15T19:44:25.733332Z","iopub.status.idle":"2025-04-15T19:44:25.743827Z","shell.execute_reply.started":"2025-04-15T19:44:25.733304Z","shell.execute_reply":"2025-04-15T19:44:25.742879Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class InsiderThreatEnv:\n    def __init__(self, X_combined, user_ids, labels):\n        self.X = X_combined    # Shape (n_samples, 29)\n        self.user_ids = user_ids\n        self.labels = labels    # Binary anomaly labels\n        self.state_builder = StateBuilder()\n        self.current_idx = 0\n    \n    def reset(self):\n        self.current_idx = np.random.randint(0, len(self.X))\n        user_id = self.user_ids[self.current_idx]\n        return self.state_builder.build_state(user_id, self.X[self.current_idx])\n    \n    def step(self, action):\n        is_anomaly = self.labels[self.current_idx]\n        \n        # Custom reward function\n        if action == 1:  # Predicted anomaly\n            reward = 15 if is_anomaly else -7  # Strong penalty for false positives\n        else:  # Predicted normal\n            reward = -20 if is_anomaly else 2   # Severe penalty for false negatives\n        \n        # Move to next sample\n        self.current_idx = (self.current_idx + 1) % len(self.X)\n        next_state = self.state_builder.build_state(\n            self.user_ids[self.current_idx],\n            self.X[self.current_idx]\n        )\n        \n        return next_state, reward, self.current_idx == 0\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:44:40.608097Z","iopub.execute_input":"2025-04-15T19:44:40.608865Z","iopub.status.idle":"2025-04-15T19:44:40.614589Z","shell.execute_reply.started":"2025-04-15T19:44:40.608839Z","shell.execute_reply":"2025-04-15T19:44:40.61375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_evaluate(X_combined, user_ids, labels):\n    # Initialize environment and agent\n    env = InsiderThreatEnv(X_combined, user_ids, labels)\n    agent = DQNAgent()\n    \n    # Training parameters\n    batch_size = 128\n    episodes = 1000\n    log_interval = 50\n    \n    # Training loop\n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        \n        while not done:\n            action = agent.act(state)\n            next_state, reward, done = env.step(action)\n            agent.remember(state, action, reward, next_state, done)\n            total_reward += reward\n            state = next_state\n            \n            if len(agent.memory) > batch_size:\n                agent.replay(batch_size)\n        \n        # Logging\n        if episode % log_interval == 0:\n            print(f\"Episode {episode:4d} | \"\n                  f\"Total Reward: {total_reward:7.1f} | \"\n                  f\"Epsilon: {agent.epsilon:.3f} | \"\n                  f\"Memory Size: {len(agent.memory)}\")\n    \n    # Evaluation\n    print(\"\\n=== Final Evaluation ===\")\n    test_episodes = 100\n    tp, fp, tn, fn = 0, 0, 0, 0\n    \n    for _ in range(test_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            action = agent.act(state, epsilon=0)  # Greedy policy\n            is_anomaly = env.labels[env.current_idx]\n            \n            if action == 1 and is_anomaly: tp += 1\n            elif action == 1 and not is_anomaly: fp += 1\n            elif action == 0 and is_anomaly: fn += 1\n            else: tn += 1\n            \n            _, _, done = env.step(action)\n    \n    precision = tp / (tp + fp + 1e-10)\n    recall = tp / (tp + fn + 1e-10)\n    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n    \n    print(f\"Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n    print(f\"Confusion Matrix:\\n[[{tn:4d} {fp:4d}]\\n [{fn:4d} {tp:4d}]]\")\n    \n    return agent\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:44:55.511039Z","iopub.execute_input":"2025-04-15T19:44:55.511342Z","iopub.status.idle":"2025-04-15T19:44:55.519248Z","shell.execute_reply.started":"2025-04-15T19:44:55.511322Z","shell.execute_reply":"2025-04-15T19:44:55.518398Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_agent = train_and_evaluate(X_combined, user_ids, is_anomaly) 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-15T19:45:22.246071Z","iopub.execute_input":"2025-04-15T19:45:22.246365Z","iopub.status.idle":"2025-04-16T01:07:51.503873Z","shell.execute_reply.started":"2025-04-15T19:45:22.246341Z","shell.execute_reply":"2025-04-16T01:07:51.502432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trained_agent.model.save('dqn_model.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:07:57.447125Z","iopub.execute_input":"2025-04-16T01:07:57.447854Z","iopub.status.idle":"2025-04-16T01:07:57.465762Z","shell.execute_reply.started":"2025-04-16T01:07:57.447826Z","shell.execute_reply":"2025-04-16T01:07:57.464748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"dqn","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('/kaggle/working/user_ids.pkl', 'rb') as f:\n    user_ids = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:30:13.541855Z","iopub.execute_input":"2025-04-16T04:30:13.542454Z","iopub.status.idle":"2025-04-16T04:30:13.562199Z","shell.execute_reply.started":"2025-04-16T04:30:13.542429Z","shell.execute_reply":"2025-04-16T04:30:13.561377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:30:16.483209Z","iopub.execute_input":"2025-04-16T04:30:16.483855Z","iopub.status.idle":"2025-04-16T04:30:16.487288Z","shell.execute_reply.started":"2025-04-16T04:30:16.483826Z","shell.execute_reply":"2025-04-16T04:30:16.486544Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, state_dim=39, action_dim=2):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=5000)  # Reduced memory size\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.model = self._build_model()\n        self.target_model = self._build_model()\n        self.update_target_network()\n    \n    def _build_model(self):\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(32, activation='relu', input_dim=self.state_dim),  # Smaller network\n            tf.keras.layers.Dense(16, activation='relu'),\n            tf.keras.layers.Dense(self.action_dim)\n        ])\n        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.005))\n        return model\n    \n    def update_target_network(self):\n        self.target_model.set_weights(self.model.get_weights())\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def act(self, state, epsilon=None):\n        if epsilon is None:\n            epsilon = self.epsilon\n        if np.random.rand() <= epsilon:\n            return random.randrange(self.action_dim)\n        q_values = self.model.predict(state[np.newaxis], verbose=0)\n        return np.argmax(q_values[0])\n    \n    def replay(self, batch_size):\n        if len(self.memory) < batch_size:\n            return\n        \n        minibatch = random.sample(self.memory, batch_size)\n        states = np.array([t[0] for t in minibatch])\n        actions = np.array([t[1] for t in minibatch])\n        rewards = np.array([t[2] for t in minibatch])\n        next_states = np.array([t[3] for t in minibatch])\n        dones = np.array([t[4] for t in minibatch])\n        \n        targets = self.model.predict(states, verbose=0)\n        next_q_values = self.target_model.predict(next_states, verbose=0)\n        \n        for i in range(batch_size):\n            targets[i][actions[i]] = rewards[i] if dones[i] else rewards[i] + self.gamma * np.amax(next_q_values[i])\n        \n        self.model.fit(states, targets, batch_size=batch_size, verbose=0)\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:30:42.452292Z","iopub.execute_input":"2025-04-16T04:30:42.452924Z","iopub.status.idle":"2025-04-16T04:30:42.462957Z","shell.execute_reply.started":"2025-04-16T04:30:42.4529Z","shell.execute_reply":"2025-04-16T04:30:42.462372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class InsiderThreatEnv:\n    def __init__(self, X_combined, user_ids, labels):\n        self.X = X_combined[:, [0,1,2,3,4,5,6,7,8,9,20,21,22,23,24,25,26,27,28]]  # First 10 features + role + 8 activity\n        self.user_ids = user_ids\n        self.labels = labels\n        self.state_builder = StateBuilder()\n        self.current_idx = 0\n    \n    def reset(self):\n        self.current_idx = np.random.randint(0, len(self.X))\n        user_id = self.user_ids[self.current_idx]\n        return self.state_builder.build_state(user_id, self.X[self.current_idx])\n    \n    def step(self, action):\n        is_anomaly = self.labels[self.current_idx]\n        \n        if action == 1:  # Predicted anomaly\n            reward = 15 if is_anomaly else -7\n        else:  # Predicted normal\n            reward = -20 if is_anomaly else 2\n        \n        self.current_idx = (self.current_idx + 1) % len(self.X)\n        next_state = self.state_builder.build_state(\n            self.user_ids[self.current_idx],\n            self.X[self.current_idx]\n        )\n        \n        return next_state, reward, self.current_idx == 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:30:59.904393Z","iopub.execute_input":"2025-04-16T04:30:59.905142Z","iopub.status.idle":"2025-04-16T04:30:59.911037Z","shell.execute_reply.started":"2025-04-16T04:30:59.905116Z","shell.execute_reply":"2025-04-16T04:30:59.910269Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[:, [0,1,2,3,4,5,6,7,8,9,20,21,22,23,24,25,26,27,28]][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:28:27.066586Z","iopub.execute_input":"2025-04-16T04:28:27.066885Z","iopub.status.idle":"2025-04-16T04:28:27.326899Z","shell.execute_reply.started":"2025-04-16T04:28:27.066865Z","shell.execute_reply":"2025-04-16T04:28:27.325986Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_evaluate(X_combined, user_ids, labels):\n    env = InsiderThreatEnv(X_combined, user_ids, labels)\n    agent = DQNAgent()\n    \n    batch_size = 64  # Smaller batch size\n    episodes = 20   # Fewer episodes\n    log_interval = 100\n    \n    for episode in range(episodes):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        \n        while not done:\n            action = agent.act(state)\n            next_state, reward, done = env.step(action)\n            agent.remember(state, action, reward, next_state, done)\n            total_reward += reward\n            state = next_state\n            \n            if len(agent.memory) > batch_size:\n                agent.replay(batch_size)\n        \n        if episode % log_interval == 0:\n            print(f\"Episode {episode:4d} | Total Reward: {total_reward:7.1f} | Epsilon: {agent.epsilon:.3f}\")\n    \n    # Simplified evaluation\n    print(\"\\n=== Final Evaluation ===\")\n    test_episodes = 50\n    tp, fp, tn, fn = 0, 0, 0, 0\n    \n    for _ in range(test_episodes):\n        state = env.reset()\n        done = False\n        while not done:\n            action = agent.act(state, epsilon=0)\n            is_anomaly = env.labels[env.current_idx]\n            \n            if action == 1 and is_anomaly: tp += 1\n            elif action == 1 and not is_anomaly: fp += 1\n            elif action == 0 and is_anomaly: fn += 1\n            else: tn += 1\n            \n            _, _, done = env.step(action)\n    \n    precision = tp / (tp + fp + 1e-10)\n    recall = tp / (tp + fn + 1e-10)\n    f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n    \n    print(f\"Precision: {precision:.3f} | Recall: {recall:.3f} | F1: {f1:.3f}\")\n    print(f\"Confusion Matrix:\\n[[{tn:4d} {fp:4d}]\\n [{fn:4d} {tp:4d}]]\")\n    \n    return agent\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:31:03.308159Z","iopub.execute_input":"2025-04-16T04:31:03.308879Z","iopub.status.idle":"2025-04-16T04:31:03.316212Z","shell.execute_reply.started":"2025-04-16T04:31:03.308854Z","shell.execute_reply":"2025-04-16T04:31:03.31554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T01:45:18.432503Z","iopub.execute_input":"2025-04-16T01:45:18.432967Z","iopub.status.idle":"2025-04-16T01:45:18.437604Z","shell.execute_reply.started":"2025-04-16T01:45:18.432945Z","shell.execute_reply":"2025-04-16T01:45:18.436975Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[:,2:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:32:09.108934Z","iopub.execute_input":"2025-04-16T04:32:09.109648Z","iopub.status.idle":"2025-04-16T04:32:09.114655Z","shell.execute_reply.started":"2025-04-16T04:32:09.1096Z","shell.execute_reply":"2025-04-16T04:32:09.114002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = X_combined[:,2:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:38:14.985477Z","iopub.execute_input":"2025-04-16T04:38:14.986213Z","iopub.status.idle":"2025-04-16T04:38:14.989457Z","shell.execute_reply.started":"2025-04-16T04:38:14.986189Z","shell.execute_reply":"2025-04-16T04:38:14.988848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = np.delete(X_combined, 20, axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:38:17.54581Z","iopub.execute_input":"2025-04-16T04:38:17.546399Z","iopub.status.idle":"2025-04-16T04:38:17.71204Z","shell.execute_reply.started":"2025-04-16T04:38:17.546377Z","shell.execute_reply":"2025-04-16T04:38:17.711125Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:38:18.586534Z","iopub.execute_input":"2025-04-16T04:38:18.587168Z","iopub.status.idle":"2025-04-16T04:38:18.591782Z","shell.execute_reply.started":"2025-04-16T04:38:18.587145Z","shell.execute_reply":"2025-04-16T04:38:18.591116Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = np.delete(X_combined, [0,1,2,7,9,10,11,13, 18, 19], axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:41:34.351633Z","iopub.execute_input":"2025-04-16T04:41:34.351929Z","iopub.status.idle":"2025-04-16T04:41:35.024277Z","shell.execute_reply.started":"2025-04-16T04:41:34.351909Z","shell.execute_reply":"2025-04-16T04:41:35.0234Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:41:37.803263Z","iopub.execute_input":"2025-04-16T04:41:37.803516Z","iopub.status.idle":"2025-04-16T04:41:37.808902Z","shell.execute_reply.started":"2025-04-16T04:41:37.803499Z","shell.execute_reply":"2025-04-16T04:41:37.808128Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"trained_agent = train_and_evaluate(X_combined, user_ids, is_anomaly)","metadata":{"execution":{"iopub.status.busy":"2025-04-16T01:48:29.411102Z","iopub.execute_input":"2025-04-16T01:48:29.411726Z","iopub.status.idle":"2025-04-16T02:14:23.422105Z","shell.execute_reply.started":"2025-04-16T01:48:29.411689Z","shell.execute_reply":"2025-04-16T02:14:23.420954Z"}}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\nwith open('user_ids.pkl', 'wb') as f:\n    pickle.dump(user_ids, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T02:15:23.015269Z","iopub.execute_input":"2025-04-16T02:15:23.016044Z","iopub.status.idle":"2025-04-16T02:15:23.031174Z","shell.execute_reply.started":"2025-04-16T02:15:23.01601Z","shell.execute_reply":"2025-04-16T02:15:23.030482Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T08:31:34.221606Z","iopub.execute_input":"2025-04-16T08:31:34.221863Z","iopub.status.idle":"2025-04-16T08:31:34.226781Z","shell.execute_reply.started":"2025-04-16T08:31:34.221845Z","shell.execute_reply":"2025-04-16T08:31:34.22596Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"ppo from colab improved","metadata":{}},{"cell_type":"code","source":"mu_all = np.load('/kaggle/working/mu_all.npy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T14:32:06.612243Z","iopub.execute_input":"2025-04-16T14:32:06.612897Z","iopub.status.idle":"2025-04-16T14:32:06.622475Z","shell.execute_reply.started":"2025-04-16T14:32:06.612876Z","shell.execute_reply":"2025-04-16T14:32:06.621814Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"is_anomaly = np.load('/kaggle/working/anomaly_labels.npy')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:59:42.297207Z","iopub.execute_input":"2025-04-16T17:59:42.297514Z","iopub.status.idle":"2025-04-16T17:59:42.304936Z","shell.execute_reply.started":"2025-04-16T17:59:42.297496Z","shell.execute_reply":"2025-04-16T17:59:42.30403Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = np.load('/kaggle/working/X_combined_new.npy', allow_pickle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:00:11.343217Z","iopub.execute_input":"2025-04-16T18:00:11.343947Z","iopub.status.idle":"2025-04-16T18:00:11.906934Z","shell.execute_reply.started":"2025-04-16T18:00:11.343922Z","shell.execute_reply":"2025-04-16T18:00:11.906391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:00:13.884942Z","iopub.execute_input":"2025-04-16T18:00:13.885226Z","iopub.status.idle":"2025-04-16T18:00:13.890265Z","shell.execute_reply.started":"2025-04-16T18:00:13.885206Z","shell.execute_reply":"2025-04-16T18:00:13.889727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = X_combined[:,2]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = AnomalyThresholdEnv(\n        recon_errors=mu_all,\n        role_features=your_role_features,\n        labels=is_anomaly\n    )\n    \n    # Train the PPO agent\n    ppo_agent = train_ppo_anomaly_detection(env, episodes=500)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:56:01.33274Z","iopub.execute_input":"2025-04-16T04:56:01.333016Z","iopub.status.idle":"2025-04-16T04:56:01.3384Z","shell.execute_reply.started":"2025-04-16T04:56:01.332996Z","shell.execute_reply":"2025-04-16T04:56:01.337516Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"user_ids.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:56:41.174482Z","iopub.execute_input":"2025-04-16T04:56:41.175083Z","iopub.status.idle":"2025-04-16T04:56:41.179599Z","shell.execute_reply.started":"2025-04-16T04:56:41.175059Z","shell.execute_reply":"2025-04-16T04:56:41.178892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"is_anomaly.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:46:22.254243Z","iopub.execute_input":"2025-04-16T04:46:22.255051Z","iopub.status.idle":"2025-04-16T04:46:22.25949Z","shell.execute_reply.started":"2025-04-16T04:46:22.255019Z","shell.execute_reply":"2025-04-16T04:46:22.258892Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.save('X_combined_new.npy', X_combined)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T04:53:50.722987Z","iopub.execute_input":"2025-04-16T04:53:50.723686Z","iopub.status.idle":"2025-04-16T04:53:51.074169Z","shell.execute_reply.started":"2025-04-16T04:53:50.723657Z","shell.execute_reply":"2025-04-16T04:53:51.073376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:07:15.348497Z","iopub.execute_input":"2025-04-16T09:07:15.348783Z","iopub.status.idle":"2025-04-16T09:07:15.352371Z","shell.execute_reply.started":"2025-04-16T09:07:15.348763Z","shell.execute_reply":"2025-04-16T09:07:15.351736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def configure_gpu():\n    gpus = tf.config.list_physical_devices('GPU')\n    if gpus:\n        try:\n            # Set memory growth before allocating any tensors\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n            print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n            return True\n        except RuntimeError as e:\n            print(f\"GPU configuration error: {e}\")\n    return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:14:54.833316Z","iopub.execute_input":"2025-04-16T05:14:54.833937Z","iopub.status.idle":"2025-04-16T05:14:54.838439Z","shell.execute_reply.started":"2025-04-16T05:14:54.833914Z","shell.execute_reply":"2025-04-16T05:14:54.837819Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gpu_available = configure_gpu()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:15:22.184284Z","iopub.execute_input":"2025-04-16T05:15:22.185055Z","iopub.status.idle":"2025-04-16T05:15:22.189Z","shell.execute_reply.started":"2025-04-16T05:15:22.18503Z","shell.execute_reply":"2025-04-16T05:15:22.188261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:37:31.786406Z","iopub.execute_input":"2025-04-16T05:37:31.7867Z","iopub.status.idle":"2025-04-16T05:37:31.792559Z","shell.execute_reply.started":"2025-04-16T05:37:31.786678Z","shell.execute_reply":"2025-04-16T05:37:31.791665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class AnomalyDetectionEnv:\n    def __init__(self, X_combined, user_ids, labels, window_size=7):\n        \"\"\"\n        X_combined: (330285, 19) array\n            - First 10: manual features\n            - Next 1: role value\n            - Last 8: LSTM-VAE latent vector\n        user_ids: (330285,) user identifiers\n        labels: (330285,) anomaly labels (1=anomaly, 0=normal)\n        window_size: how many past days to include in state\n        \"\"\"\n        self.X = X_combined\n        self.user_ids = user_ids\n        self.labels = labels\n        self.window_size = window_size\n        self.current_idx = window_size  # Start with enough history\n        \n        # Track current user's history\n        self.current_user = user_ids[window_size]\n        self.user_history = deque(maxlen=window_size)\n        for i in range(window_size):\n            if user_ids[i] == self.current_user:\n                self.user_history.append(X_combined[i])\n        \n        # Action space: 0 (normal) or 1 (anomaly)\n        self.action_space = 2  \n        \n        # State: current features + historical mean + user role\n        self.state_dim = X_combined.shape[1] + X_combined.shape[1]\n\n    def reset(self):\n        \"\"\"Reset to start new episode\"\"\"\n        self.current_idx = self.window_size\n        self.current_user = self.user_ids[self.window_size]\n        self.user_history = deque(maxlen=self.window_size)\n        for i in range(self.window_size):\n            if self.user_ids[i] == self.current_user:\n                self.user_history.append(self.X[i])\n        return self._get_state()\n\n    def _get_state(self):\n        \"\"\"Create state vector\"\"\"\n        current = self.X[self.current_idx]\n        history_mean = np.mean(self.user_history, axis=0) if self.user_history else np.zeros_like(current)\n        role = current[10]  # Role feature is at index 10\n        return np.concatenate([current, history_mean, [role]]).astype(np.float32)\n\n    def step(self, action):\n        \"\"\"\n        action: 0 (normal) or 1 (anomaly)\n        Returns: next_state, reward, done\n        \"\"\"\n        true_label = self.labels[self.current_idx]\n        \n        # Custom reward function\n        if action == 1:  # Predicted anomaly\n            reward = 1.0 if true_label == 1 else -0.5\n        else:  # Predicted normal\n            reward = -1.0 if true_label == 1 else 0.1\n        \n        # Update position and history\n        self.current_idx += 1\n        if self.current_idx >= len(self.X):\n            done = True\n            next_state = self.reset()\n        else:\n            done = False\n            # Update user history if same user\n            if self.user_ids[self.current_idx] == self.current_user:\n                self.user_history.append(self.X[self.current_idx-1])\n            else:\n                self.current_user = self.user_ids[self.current_idx]\n                self.user_history = deque(maxlen=self.window_size)\n            next_state = self._get_state()\n        \n        return next_state, reward, done\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:38:05.967509Z","iopub.execute_input":"2025-04-16T05:38:05.968139Z","iopub.status.idle":"2025-04-16T05:38:05.977955Z","shell.execute_reply.started":"2025-04-16T05:38:05.968117Z","shell.execute_reply":"2025-04-16T05:38:05.977242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:00:22.567134Z","iopub.execute_input":"2025-04-16T18:00:22.567501Z","iopub.status.idle":"2025-04-16T18:00:22.573798Z","shell.execute_reply.started":"2025-04-16T18:00:22.567476Z","shell.execute_reply":"2025-04-16T18:00:22.572463Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nclass AnomalyDetectionEnv:\n    def __init__(self, X_combined, user_ids, labels, window_size=7):\n        \"\"\"\n        X_combined: (330285, 19) array\n            - First 10: manual features\n            - Next 1: role value\n            - Last 8: LSTM-VAE latent vector\n        user_ids: (330285,) user identifiers\n        labels: (330285,) anomaly labels (1=anomaly, 0=normal)\n        window_size: how many past days to include in state\n        \"\"\"\n        self.X = X_combined.astype(np.float32)  # Ensure float32 for efficiency\n        self.user_ids = user_ids\n        self.labels = labels.astype(np.int32)\n        self.window_size = window_size\n        self.current_idx = window_size\n        self.n_features = X_combined.shape[1]\n        \n        # Track current user's history\n        self.current_user = user_ids[window_size]\n        self.user_history = deque(maxlen=window_size)\n        for i in range(window_size):\n            if user_ids[i] == self.current_user:\n                self.user_history.append(X_combined[i])\n        \n        # Action space: 0 (normal) or 1 (anomaly)\n        self.action_space = 2\n        \n        # State: current features + historical mean\n        self.state_dim = self.n_features + self.n_features\n\n    def reset(self):\n        \"\"\"Reset to start new episode\"\"\"\n        self.current_idx = self.window_size\n        self.current_user = self.user_ids[self.window_size]\n        self.user_history = deque(maxlen=self.window_size)\n        for i in range(self.window_size):\n            if self.user_ids[i] == self.current_user:\n                self.user_history.append(self.X[i])\n        return self._get_state()\n\n    def _get_state(self):\n        \"\"\"Create state vector\"\"\"\n        current = self.X[self.current_idx]\n        history_mean = np.mean(list(self.user_history), axis=0) if self.user_history else np.zeros(self.n_features, dtype=np.float32)\n        return np.concatenate([current, history_mean]).astype(np.float32)\n\n    def step(self, action):\n        \"\"\"\n        action: 0 (normal) or 1 (anomaly)\n        Returns: next_state, reward, done\n        \"\"\"\n        true_label = self.labels[self.current_idx]\n        \n        # Optimized reward function\n        reward = (\n            1.0 if (action == 1 and true_label == 1) else\n            -0.5 if (action == 1 and true_label == 0) else\n            -1.0 if (action == 0 and true_label == 1) else\n            0.1\n        )\n        \n        # Update position\n        self.current_idx += 1\n        done = self.current_idx >= len(self.X)\n        \n        if done:\n            next_state = self.reset()\n        else:\n            # Update user history efficiently\n            if self.user_ids[self.current_idx] == self.current_user:\n                self.user_history.append(self.X[self.current_idx-1])\n            else:\n                self.current_user = self.user_ids[self.current_idx]\n                self.user_history = deque(maxlen=self.window_size)\n                self.user_history.append(self.X[self.current_idx-1])\n            next_state = self._get_state()\n        \n        return next_state, reward, done\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:33:20.129165Z","iopub.execute_input":"2025-04-16T05:33:20.129908Z","iopub.status.idle":"2025-04-16T05:33:20.140639Z","shell.execute_reply.started":"2025-04-16T05:33:20.129884Z","shell.execute_reply":"2025-04-16T05:33:20.139775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class DQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=10000)\n        self.gamma = 0.95\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.995\n        self.batch_size = 256\n        self.model = self._build_model()\n        self.target_model = self._build_model()  # Add target network\n        self.update_target_model()\n        \n    def _build_model(self):\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(64, activation='relu', input_dim=self.state_dim,\n                                kernel_initializer='he_uniform'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dense(32, activation='relu',\n                                kernel_initializer='he_uniform'),\n            tf.keras.layers.BatchNormalization(),\n            tf.keras.layers.Dense(self.action_dim, activation='linear')\n        ])\n        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001))\n        return model\n    \n    def update_target_model(self):\n        \"\"\"Update target network weights\"\"\"\n        self.target_model.set_weights(self.model.get_weights())\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_dim)\n        state = state.reshape(1, -1)\n        q_values = self.model.predict(state, verbose=0)[0]\n        return np.argmax(q_values)\n    \n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n        \n        minibatch = random.sample(self.memory, self.batch_size)\n        \n        # Vectorized data preparation\n        states = np.array([x[0] for x in minibatch])\n        actions = np.array([x[1] for x in minibatch])\n        rewards = np.array([x[2] for x in minibatch])\n        next_states = np.array([x[3] for x in minibatch])\n        dones = np.array([x[4] for x in minibatch])\n        \n        # Double DQN\n        targets = self.model.predict(states, verbose=0)\n        next_q_values = self.target_model.predict(next_states, verbose=0)\n        \n        targets[range(self.batch_size), actions] = rewards + (1 - dones) * self.gamma * np.max(next_q_values, axis=1)\n        \n        self.model.fit(states, targets, batch_size=self.batch_size, verbose=0)\n        \n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:33:38.60423Z","iopub.execute_input":"2025-04-16T05:33:38.605023Z","iopub.status.idle":"2025-04-16T05:33:38.615257Z","shell.execute_reply.started":"2025-04-16T05:33:38.60498Z","shell.execute_reply":"2025-04-16T05:33:38.614677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_dqn(X_combined, user_ids, labels, episodes=10):\n    env = AnomalyDetectionEnv(X_combined, user_ids, labels)\n    agent = DQNAgent(env.state_dim, env.action_space)\n    \n    rewards_history = []\n    target_update_freq = 10  # Update target network every 10 steps\n    \n    for episode in tqdm(range(episodes), desc=\"Training DQN\"):\n        state = env.reset()\n        total_reward = 0\n        done = False\n        step = 0\n        \n        while not done:\n            action = agent.act(state)\n            next_state, reward, done = env.step(action)\n            agent.remember(state, action, reward, next_state, done)\n            state = next_state\n            total_reward += reward\n            \n            if len(agent.memory) > agent.batch_size:\n                agent.replay()\n            \n            step += 1\n            if step % target_update_freq == 0:\n                agent.update_target_model()\n        \n        rewards_history.append(total_reward)\n        \n        if episode % 5 == 0:\n            print(f\"Episode {episode}: Reward={total_reward:.1f}, Epsilon={agent.epsilon:.2f}\")\n        \n        # Memory cleanup\n        gc.collect()\n    \n    return agent, rewards_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:34:17.777138Z","iopub.execute_input":"2025-04-16T05:34:17.77741Z","iopub.status.idle":"2025-04-16T05:34:17.783958Z","shell.execute_reply.started":"2025-04-16T05:34:17.777392Z","shell.execute_reply":"2025-04-16T05:34:17.783157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent, rewards_history = train_dqn(X_combined, user_ids, is_anomaly, episodes=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:34:49.57815Z","iopub.execute_input":"2025-04-16T05:34:49.578739Z","iopub.status.idle":"2025-04-16T05:37:08.476445Z","shell.execute_reply.started":"2025-04-16T05:34:49.578715Z","shell.execute_reply":"2025-04-16T05:37:08.475347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class SimpleAnomalyEnv:\n    def __init__(self, X_combined, user_ids, labels):\n        \"\"\"\n        Simplified environment using:\n        - Last 8 columns (latent vectors) as features\n        - Column 10 (role) as additional feature\n        \"\"\"\n        self.X = X_combined\n        self.user_ids = user_ids\n        self.labels = labels\n        self.current_idx = 0\n        \n        # Use last 8 columns + role (column 10)\n        self.state_dim = 8 + 1  # 8 latent + 1 role\n        self.action_space = 2  # 0=normal, 1=anomaly\n\n    def reset(self):\n        self.current_idx = random.randint(0, len(self.X)-1)\n        return self._get_state()\n\n    def _get_state(self):\n        \"\"\"State: last 8 features + role\"\"\"\n        latent = self.X[self.current_idx][-8:]  # Last 8 columns\n        role = np.array([self.X[self.current_idx][10]])  # Role as array\n        return np.concatenate([latent, role]).astype(np.float32)\n\n    def step(self, action):\n        true_label = self.labels[self.current_idx]\n        \n        # Simple reward function\n        if action == 1:  # Predict anomaly\n            reward = 1.0 if true_label == 1 else -0.5\n        else:  # Predict normal\n            reward = -1.0 if true_label == 1 else 0.1\n        \n        self.current_idx = (self.current_idx + 1) % len(self.X)\n        done = False  # Continuous training\n        \n        return self._get_state(), reward, done\n\nclass FastDQNAgent:\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=5000)\n        self.gamma = 0.9\n        self.epsilon = 1.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.98\n        self.batch_size = 128\n        \n        # Tiny neural network\n        self.model = tf.keras.Sequential([\n            tf.keras.layers.Dense(32, activation='relu', input_dim=state_dim),\n            tf.keras.layers.Dense(action_dim)\n        ])\n        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.001))\n\n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_dim)\n        q_values = self.model.predict(state[np.newaxis], verbose=0)\n        return np.argmax(q_values[0])\n    \n    def remember(self, state, action, reward, next_state, done):\n        self.memory.append((state, action, reward, next_state, done))\n    \n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n        \n        # Vectorized batch processing\n        minibatch = random.sample(self.memory, self.batch_size)\n        states = np.array([x[0] for x in minibatch])\n        actions = np.array([x[1] for x in minibatch])\n        rewards = np.array([x[2] for x in minibatch])\n        next_states = np.array([x[3] for x in minibatch])\n        \n        # Single forward pass\n        targets = rewards + self.gamma * np.amax(self.model.predict(next_states, verbose=0), axis=1)\n        target_vec = self.model.predict(states, verbose=0)\n        target_vec[np.arange(self.batch_size), actions] = targets\n        \n        # Train on batch\n        self.model.train_on_batch(states, target_vec)\n        \n        # Decay epsilon\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:39:21.640224Z","iopub.execute_input":"2025-04-16T05:39:21.640492Z","iopub.status.idle":"2025-04-16T05:39:21.653235Z","shell.execute_reply.started":"2025-04-16T05:39:21.640474Z","shell.execute_reply":"2025-04-16T05:39:21.652638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_fast_dqn(X_combined, user_ids, labels, steps=50000):\n    env = SimpleAnomalyEnv(X_combined, user_ids, labels)\n    agent = FastDQNAgent(env.state_dim, env.action_space)\n    \n    state = env.reset()\n    reward_history = []\n    \n    for step in tqdm(range(steps), desc=\"Training\"):\n        action = agent.act(state)\n        next_state, reward, done = env.step(action)\n        agent.remember(state, action, reward, next_state, done)\n        state = next_state\n        reward_history.append(reward)\n        \n        if len(agent.memory) > agent.batch_size:\n            agent.replay()\n        \n        if step % 1000 == 0:\n            avg_reward = np.mean(reward_history[-1000:]) if len(reward_history) > 1000 else 0\n            print(f\"Step {step}: Avg Reward={avg_reward:.2f}, Epsilon={agent.epsilon:.2f}\")\n    \n    return agent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:45:55.357249Z","iopub.execute_input":"2025-04-16T05:45:55.357804Z","iopub.status.idle":"2025-04-16T05:45:55.363853Z","shell.execute_reply.started":"2025-04-16T05:45:55.357782Z","shell.execute_reply":"2025-04-16T05:45:55.363086Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T05:45:35.027045Z","iopub.execute_input":"2025-04-16T05:45:35.027342Z","iopub.status.idle":"2025-04-16T05:45:35.051924Z","shell.execute_reply.started":"2025-04-16T05:45:35.027323Z","shell.execute_reply":"2025-04-16T05:45:35.051359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent = train_fast_dqn(X_combined, user_ids, is_anomaly, steps=5000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:05:22.785103Z","iopub.execute_input":"2025-04-16T06:05:22.785441Z","iopub.status.idle":"2025-04-16T06:39:03.649015Z","shell.execute_reply.started":"2025-04-16T06:05:22.785419Z","shell.execute_reply":"2025-04-16T06:39:03.64831Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent.model.save('dqn_model_1.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:39:03.650209Z","iopub.execute_input":"2025-04-16T06:39:03.650434Z","iopub.status.idle":"2025-04-16T06:39:03.695267Z","shell.execute_reply.started":"2025-04-16T06:39:03.650417Z","shell.execute_reply":"2025-04-16T06:39:03.694731Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:07:03.757556Z","iopub.execute_input":"2025-04-16T09:07:03.75818Z","iopub.status.idle":"2025-04-16T09:07:03.76162Z","shell.execute_reply.started":"2025-04-16T09:07:03.758157Z","shell.execute_reply":"2025-04-16T09:07:03.760914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FastDQNAgent class (simplified for evaluation)\nclass FastDQNAgent:\n    def __init__(self, state_dim, action_dim, model_path):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        # Specify custom_objects to map 'mse' to the correct loss function\n        self.model = tf.keras.models.load_model(\n            model_path,\n            custom_objects={'mse': tf.keras.losses.MeanSquaredError}\n        )\n        self.epsilon = 0.0  # No exploration during evaluation\n\n    def act(self, state):\n        q_values = self.model.predict(state[np.newaxis], verbose=0)[0]\n        return np.argmax(q_values), q_values[1]  # Action and Q-value for action 1\n\n# Evaluation function with sampling\ndef evaluate_dqn_agent(env, agent, is_anomaly, sample_size=10000):\n    # Randomly sample indices\n    np.random.seed(42)  # For reproducibility\n    sample_indices = np.random.choice(len(is_anomaly), size=sample_size, replace=False)\n    sample_indices = np.sort(sample_indices)  # Sort for sequential processing\n    \n    predictions = []\n    true_labels = []\n    anomaly_scores = []  # Q-values for action 1 (for AUC-ROC)\n    \n    for idx in sample_indices:\n        env.current_idx = idx\n        state = env._get_state()\n        action, anomaly_score = agent.act(state)\n        predictions.append(action)\n        anomaly_scores.append(anomaly_score)\n        true_labels.append(is_anomaly[idx])\n    \n    accuracy = accuracy_score(true_labels, predictions)\n    precision = precision_score(true_labels, predictions)\n    recall = recall_score(true_labels, predictions)\n    f1 = f1_score(true_labels, predictions)\n    auc_roc = roc_auc_score(true_labels, anomaly_scores)\n    cm = confusion_matrix(true_labels, predictions)\n    tn, fp, fn, tp = cm.ravel()\n    \n    a = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'auc_roc': auc_roc,\n        'false_negatives': fn,\n        'true_positives': tp,\n        'false_positives': fp,\n        'true_negatives': tn,\n        'confusion_matrix': cm\n    }\n    \n    return a","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:53:09.328447Z","iopub.execute_input":"2025-04-16T06:53:09.32905Z","iopub.status.idle":"2025-04-16T06:53:09.337161Z","shell.execute_reply.started":"2025-04-16T06:53:09.329025Z","shell.execute_reply":"2025-04-16T06:53:09.336235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize environment\nenv = SimpleAnomalyEnv(X_combined, user_ids, is_anomaly)\n    \n    # Load saved agent\nmodel_path = '/kaggle/working/dqn_model_1.h5'\nagent = FastDQNAgent(env.state_dim, env.action_space, model_path=model_path)\n    \n# Evaluate on sample\nmetrics = evaluate_dqn_agent(env, agent, is_anomaly, sample_size=10000)\nprint(\"\\nEvaluation Metrics (Sampled Data):\")\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T06:53:14.112208Z","iopub.execute_input":"2025-04-16T06:53:14.11289Z","iopub.status.idle":"2025-04-16T07:05:12.399668Z","shell.execute_reply.started":"2025-04-16T06:53:14.112866Z","shell.execute_reply":"2025-04-16T07:05:12.398743Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"trying to handle imbalance with class weights","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:06:58.99957Z","iopub.execute_input":"2025-04-16T09:06:59.000207Z","iopub.status.idle":"2025-04-16T09:06:59.003429Z","shell.execute_reply.started":"2025-04-16T09:06:59.000184Z","shell.execute_reply":"2025-04-16T09:06:59.002841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = np.load('/kaggle/working/X_combined_new.npy', allow_pickle=True)\nis_anomaly = np.load('/kaggle/working/anomaly_labels.npy')\n\n\nwith open('/kaggle/working/user_ids.pkl', 'rb') as f:\n    user_ids = pickle.load(f)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:00:35.953664Z","iopub.execute_input":"2025-04-16T18:00:35.954525Z","iopub.status.idle":"2025-04-16T18:00:36.422947Z","shell.execute_reply.started":"2025-04-16T18:00:35.954496Z","shell.execute_reply":"2025-04-16T18:00:36.422326Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FullFeatureAnomalyEnv:\n    def __init__(self, X_combined, user_ids, labels):\n        \"\"\"\n        Environment using all 19 features:\n        - Indices 0-9: Statistical (manual) features\n        - Index 10: Role\n        - Indices 11-18: LSTM autoencoder latent vectors\n        \"\"\"\n        if X_combined.shape[1] != 19:\n            raise ValueError(f\"Expected 19 features, got {X_combined.shape[1]}\")\n        self.X = X_combined\n        self.user_ids = user_ids\n        self.labels = labels\n        self.current_idx = 0\n        self.state_dim = 19  # 10 stat + 1 role + 8 latent\n        self.action_space = 2\n        print(\"Initialized environment with state_dim=19, including:\")\n        print(\"- First 10 statistical features (indices 0-9)\")\n        print(\"- Role feature (index 10)\")\n        print(\"- 8 latent vectors (indices 11-18)\")\n\n    def reset(self):\n        self.current_idx = random.randint(0, len(self.X)-1)\n        return self._get_state()\n\n    def _get_state(self):\n        return self.X[self.current_idx].astype(np.float32)\n\n    def step(self, action):\n        true_label = self.labels[self.current_idx]\n        if action == 1:\n            reward = 100.0 if true_label == 1 else -10.0\n        else:\n            reward = -20.0 if true_label == 1 else 1.0\n        self.current_idx = (self.current_idx + 1) % len(self.X)\n        done = False\n        return self._get_state(), reward, done, self.user_ids[self.current_idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:51:14.252432Z","iopub.execute_input":"2025-04-16T09:51:14.25275Z","iopub.status.idle":"2025-04-16T09:51:14.25954Z","shell.execute_reply.started":"2025-04-16T09:51:14.25272Z","shell.execute_reply":"2025-04-16T09:51:14.258807Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FastDQNAgent:\n    def __init__(self, state_dim, action_dim, user_ids, labels, model_path=None):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=5000)\n        self.user_priorities = deque(maxlen=5000)\n        self.gamma = 0.9\n        self.epsilon = 1.0 if not model_path else 0.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.999\n        self.batch_size = 128\n        \n        # User-specific anomaly rates\n        self.user_anomaly_rates = {}\n        unique_users = np.unique(user_ids)\n        for user in unique_users:\n            user_mask = user_ids == user\n            rate = np.mean(labels[user_mask]) if np.sum(user_mask) > 0 else 0.05\n            self.user_anomaly_rates[user] = rate\n        \n        if model_path:\n            self.model = tf.keras.models.load_model(\n                model_path,\n                custom_objects={'mse': tf.keras.losses.MeanSquaredError()}\n            )\n        else:\n            self.model = self._build_model()\n        print(f\"Model initialized with input shape (None, {state_dim})\")\n\n    def _build_model(self):\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(64, activation='relu', input_dim=self.state_dim),\n            tf.keras.layers.Dense(32, activation='relu'),\n            tf.keras.layers.Dense(self.action_dim)\n        ])\n        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.001))\n        return model\n\n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_dim)\n        q_values = self.model.predict(state[np.newaxis], verbose=0)[0]\n        return np.argmax(q_values)\n\n    def act_batch(self, states):\n        q_values = self.model.predict(states, verbose=0)\n        return q_values\n\n    def remember(self, state, action, reward, next_state, done, user_id):\n        rate = self.user_anomaly_rates[user_id] / 0.05\n        priority = max(1e-3, abs(reward) * rate)  # Ensure positive priority\n        self.memory.append((state, action, reward, next_state, done))\n        self.user_priorities.append(priority)\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n        priorities = np.array(self.user_priorities)\n        priorities = np.clip(priorities, 1e-3, None)  # Prevent zeros/negatives\n        priority_sum = max(priorities.sum(), 1e-6)\n        probs = priorities / priority_sum\n        try:\n            indices = np.random.choice(len(self.memory), self.batch_size, p=probs)\n        except ValueError:\n            print(\"Warning: Invalid probabilities, using uniform sampling\")\n            indices = np.random.choice(len(self.memory), self.batch_size)\n        minibatch = [self.memory[i] for i in indices]\n        \n        states = np.array([x[0] for x in minibatch])\n        actions = np.array([x[1] for x in minibatch])\n        rewards = np.array([x[2] for x in minibatch])\n        next_states = np.array([x[3] for x in minibatch])\n        \n        targets = rewards + self.gamma * np.amax(self.model.predict(next_states, verbose=0), axis=1)\n        target_vec = self.model.predict(states, verbose=0)\n        target_vec[np.arange(self.batch_size), actions] = targets\n        \n        self.model.train_on_batch(states, target_vec)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:51:37.178031Z","iopub.execute_input":"2025-04-16T09:51:37.178295Z","iopub.status.idle":"2025-04-16T09:51:37.193425Z","shell.execute_reply.started":"2025-04-16T09:51:37.178273Z","shell.execute_reply":"2025-04-16T09:51:37.192734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_fast_dqn(X_combined, user_ids, labels, steps=10000):\n    env = FullFeatureAnomalyEnv(X_combined, user_ids, labels)\n    agent = FastDQNAgent(env.state_dim, env.action_space, user_ids, labels)\n    \n    state = env.reset()\n    reward_history = []\n    \n    for step in tqdm(range(steps), desc=\"Training\"):\n        action = agent.act(state)\n        next_state, reward, done, user_id = env.step(action)\n        agent.remember(state, action, reward, next_state, done, user_id)\n        state = next_state\n        reward_history.append(reward)\n        \n        if len(agent.memory) > agent.batch_size:\n            agent.replay()\n        \n        if agent.epsilon > agent.epsilon_min:\n            agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n        \n        if step % 1000 == 0:\n            avg_reward = np.mean(reward_history[-1000:]) if reward_history else 0\n            print(f\"Step {step}: Avg Reward={avg_reward:.2f}, Epsilon={agent.epsilon:.3f}\")\n    \n    agent.model.save('/kaggle/working/dqn_model_retrained.h5')\n    return agent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:51:49.016339Z","iopub.execute_input":"2025-04-16T09:51:49.016938Z","iopub.status.idle":"2025-04-16T09:51:49.022551Z","shell.execute_reply.started":"2025-04-16T09:51:49.016914Z","shell.execute_reply":"2025-04-16T09:51:49.021946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_dqn_agent(env, agent, is_anomaly, user_ids, sample_size=10000):\n    np.random.seed(42)\n    unique_users = np.unique(user_ids)\n    indices = []\n    samples_per_user = max(1, sample_size // len(unique_users))\n    print(f\"Sampling ~{samples_per_user} points per user across {len(unique_users)} users\")\n    for user in unique_users:\n        user_indices = np.where(user_ids == user)[0]\n        if len(user_indices) > samples_per_user:\n            sampled = np.random.choice(user_indices, samples_per_user, replace=False)\n        else:\n            sampled = user_indices\n        indices.extend(sampled)\n    indices = np.array(indices)\n    if len(indices) > sample_size:\n        indices = np.random.choice(indices, sample_size, replace=False)\n    elif len(indices) < sample_size:\n        remaining = np.random.choice(np.arange(len(is_anomaly)), sample_size - len(indices), replace=False)\n        indices = np.concatenate([indices, remaining])\n    indices = np.sort(indices)\n    \n    user_anomaly_rates = {}\n    for user in unique_users:\n        user_mask = user_ids == user\n        rate = np.mean(is_anomaly[user_mask]) if np.sum(user_mask) > 0 else 0.05\n        user_anomaly_rates[user] = rate\n    print(\"Computed user-specific anomaly rates (sample):\")\n    print({k: f\"{v:.3f}\" for k, v in list(user_anomaly_rates.items())[:5]})\n    \n    states = env.X[indices].astype(np.float32)\n    print(f\"Prepared {len(states)} states with shape {states.shape}, using all 19 features\")\n    true_labels = is_anomaly[indices]\n    sample_user_ids = user_ids[indices]\n    q_values = agent.act_batch(states)\n    anomaly_scores = q_values[:, 1]\n    \n    predictions = np.zeros_like(anomaly_scores, dtype=np.int32)\n    base_threshold = np.median(anomaly_scores)\n    for i, user_id in enumerate(sample_user_ids):\n        rate = user_anomaly_rates[user_id]\n        threshold = base_threshold * (1 - 0.5 * (rate / 0.05))\n        predictions[i] = 1 if anomaly_scores[i] > threshold else 0\n    \n    accuracy = accuracy_score(true_labels, predictions)\n    precision = precision_score(true_labels, predictions, zero_division=0)\n    recall = recall_score(true_labels, predictions, zero_division=0)\n    f1 = f1_score(true_labels, predictions, zero_division=0)\n    auc_roc = roc_auc_score(true_labels, anomaly_scores)\n    cm = confusion_matrix(true_labels, predictions)\n    tn, fp, fn, tp = cm.ravel()\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'auc_roc': auc_roc,\n        'false_negatives': fn,\n        'true_positives': tp,\n        'false_positives': fp,\n        'true_negatives': tn,\n        'confusion_matrix': cm.tolist()\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:52:09.299732Z","iopub.execute_input":"2025-04-16T09:52:09.300002Z","iopub.status.idle":"2025-04-16T09:52:09.311582Z","shell.execute_reply.started":"2025-04-16T09:52:09.299981Z","shell.execute_reply":"2025-04-16T09:52:09.310904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training Fast DQN...\")\nagent = train_fast_dqn(X_combined, user_ids, is_anomaly, steps=5000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T09:52:25.075653Z","iopub.execute_input":"2025-04-16T09:52:25.075947Z","iopub.status.idle":"2025-04-16T10:28:05.303434Z","shell.execute_reply.started":"2025-04-16T09:52:25.075927Z","shell.execute_reply":"2025-04-16T10:28:05.302587Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:20:43.582936Z","iopub.execute_input":"2025-04-16T19:20:43.583891Z","iopub.status.idle":"2025-04-16T19:20:43.590749Z","shell.execute_reply.started":"2025-04-16T19:20:43.58386Z","shell.execute_reply":"2025-04-16T19:20:43.589958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = FullFeatureAnomalyEnv(X_combined, user_ids, is_anomaly)\nmetrics = evaluate_dqn_agent(env, agent, is_anomaly, user_ids, sample_size=10000)\nprint(\"\\nEvaluation Metrics (User-Aware, Stratified Sample):\")\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T10:30:41.528494Z","iopub.execute_input":"2025-04-16T10:30:41.529128Z","iopub.status.idle":"2025-04-16T10:31:27.041282Z","shell.execute_reply.started":"2025-04-16T10:30:41.529101Z","shell.execute_reply":"2025-04-16T10:31:27.040656Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = FullFeatureAnomalyEnv(X_combined, user_ids, is_anomaly)\nmetrics = evaluate_dqn_agent(env, agent, is_anomaly, user_ids, sample_size=10000)\nprint(\"\\nEvaluation Metrics (User-Aware, Stratified Sample):\")\nprint(metrics)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent.model.save('dqn_agent_2.h5')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T10:36:43.05289Z","iopub.execute_input":"2025-04-16T10:36:43.053174Z","iopub.status.idle":"2025-04-16T10:36:43.076361Z","shell.execute_reply.started":"2025-04-16T10:36:43.053156Z","shell.execute_reply":"2025-04-16T10:36:43.075863Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"above makes model biased towards anomalies, trying to get a balance","metadata":{}},{"cell_type":"code","source":"class FullFeatureAnomalyEnv:\n    def __init__(self, X_combined, user_ids, labels):\n        \"\"\"\n        Environment using all 19 features:\n        - Indices 0-9: Statistical (manual) features\n        - Index 10: Role\n        - Indices 11-18: LSTM autoencoder latent vectors\n        \"\"\"\n        if X_combined.shape[1] != 19:\n            raise ValueError(f\"Expected 19 features, got {X_combined.shape[1]}\")\n        self.X = X_combined\n        self.user_ids = user_ids\n        self.labels = labels\n        self.current_idx = 0\n        self.state_dim = 19\n        self.action_space = 2\n        print(\"Initialized environment with state_dim=19, including:\")\n        print(\"- First 10 statistical features (indices 0-9)\")\n        print(\"- Role feature (index 10)\")\n        print(\"- 8 latent vectors (indices 11-18)\")\n\n    def reset(self):\n        self.current_idx = random.randint(0, len(self.X)-1)\n        return self._get_state()\n\n    def _get_state(self):\n        return self.X[self.current_idx].astype(np.float32)\n\n    def step(self, action):\n        true_label = self.labels[self.current_idx]\n        if action == 1:\n            reward = 40.0 if true_label == 1 else -10.0\n        else:\n            reward = -20.0 if true_label == 1 else 30.0\n        self.current_idx = (self.current_idx + 1) % len(self.X)\n        done = False\n        return self._get_state(), reward, done, self.user_ids[self.current_idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T14:31:35.870342Z","iopub.execute_input":"2025-04-16T14:31:35.870963Z","iopub.status.idle":"2025-04-16T14:31:35.877232Z","shell.execute_reply.started":"2025-04-16T14:31:35.870944Z","shell.execute_reply":"2025-04-16T14:31:35.876471Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FastDQNAgent class\nclass FastDQNAgent:\n    def __init__(self, state_dim, action_dim, user_ids, labels, model_path=None):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=5000)\n        self.user_priorities = deque(maxlen=5000)\n        self.gamma = 0.9\n        self.epsilon = 1.0 if not model_path else 0.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.999\n        self.batch_size = 128\n        \n        self.user_anomaly_rates = {}\n        unique_users = np.unique(user_ids)\n        for user in unique_users:\n            user_mask = user_ids == user\n            rate = np.mean(labels[user_mask]) if np.sum(user_mask) > 0 else 0.005\n            self.user_anomaly_rates[user] = min(rate, 0.05)  # Cap at 5%\n        \n        if model_path:\n            self.model = tf.keras.models.load_model(\n                model_path,\n                custom_objects={'mse': tf.keras.losses.MeanSquaredError()}\n            )\n        else:\n            self.model = self._build_model()\n        print(f\"Model initialized with input shape (None, {state_dim})\")\n\n    def _build_model(self):\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(64, activation='relu', input_dim=self.state_dim),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(32, activation='relu'),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(self.action_dim)\n        ])\n        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.001))\n        return model\n\n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_dim)\n        q_values = self.model.predict(state[np.newaxis], verbose=0)[0]\n        return np.argmax(q_values)\n\n    def act_batch(self, states):\n        q_values = self.model.predict(states, verbose=0)\n        return q_values\n\n    def remember(self, state, action, reward, next_state, done, user_id):\n        rate = self.user_anomaly_rates[user_id] / 0.005\n        priority = max(1e-2, abs(reward) * rate)\n        self.memory.append((state, action, reward, next_state, done))\n        self.user_priorities.append(priority)\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n        priorities = np.array(self.user_priorities)\n        priorities = np.clip(priorities, 1e-2, None)\n        priority_sum = max(priorities.sum(), 1e-6)\n        probs = priorities / priority_sum\n        try:\n            indices = np.random.choice(len(self.memory), self.batch_size, p=probs)\n        except ValueError:\n            print(\"Warning: Invalid probabilities, using uniform sampling\")\n            indices = np.random.choice(len(self.memory), self.batch_size)\n        minibatch = [self.memory[i] for i in indices]\n        \n        states = np.array([x[0] for x in minibatch])\n        actions = np.array([x[1] for x in minibatch])\n        rewards = np.array([x[2] for x in minibatch])\n        next_states = np.array([x[3] for x in minibatch])\n        \n        targets = rewards + self.gamma * np.amax(self.model.predict(next_states, verbose=0), axis=1)\n        target_vec = self.model.predict(states, verbose=0)\n        target_vec[np.arange(self.batch_size), actions] = targets\n        \n        self.model.train_on_batch(states, target_vec)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T14:31:43.466962Z","iopub.execute_input":"2025-04-16T14:31:43.467221Z","iopub.status.idle":"2025-04-16T14:31:43.479477Z","shell.execute_reply.started":"2025-04-16T14:31:43.467204Z","shell.execute_reply":"2025-04-16T14:31:43.47875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training function\ndef train_fast_dqn(X_combined, user_ids, labels, steps=20000):\n    env = FullFeatureAnomalyEnv(X_combined, user_ids, labels)\n    agent = FastDQNAgent(env.state_dim, env.action_space, user_ids, labels)\n    \n    state = env.reset()\n    reward_history = []\n    \n    for step in tqdm(range(steps), desc=\"Training\"):\n        action = agent.act(state)\n        next_state, reward, done, user_id = env.step(action)\n        agent.remember(state, action, reward, next_state, done, user_id)\n        state = next_state\n        reward_history.append(reward)\n        \n        if len(agent.memory) > agent.batch_size:\n            agent.replay()\n        \n        if agent.epsilon > agent.epsilon_min:\n            agent.epsilon = max(agent.epsilon_min, agent.epsilon * agent.epsilon_decay)\n        \n        if step % 1000 == 0:\n            avg_reward = np.mean(reward_history[-1000:]) if reward_history else 0\n            print(f\"Step {step}: Avg Reward={avg_reward:.2f}, Epsilon={agent.epsilon:.3f}\")\n    \n    agent.model.save('/kaggle/working/dqn_model_2.h5')\n    return agent\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T14:31:45.250578Z","iopub.execute_input":"2025-04-16T14:31:45.250816Z","iopub.status.idle":"2025-04-16T14:31:45.256791Z","shell.execute_reply.started":"2025-04-16T14:31:45.2508Z","shell.execute_reply":"2025-04-16T14:31:45.256075Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation function with anomaly oversampling\ndef evaluate_dqn_agent(env, agent, is_anomaly, user_ids, sample_size=10000, target_anomaly_rate=0.05):\n    np.random.seed(42)\n    # Oversample anomalies\n    anomaly_indices = np.where(is_anomaly == 1)[0]\n    normal_indices = np.where(is_anomaly == 0)[0]\n    n_anomalies = int(sample_size * target_anomaly_rate)  # e.g., 500 for 5%\n    n_normal = sample_size - n_anomalies\n    \n    # Stratified sampling per user\n    unique_users = np.unique(user_ids)\n    samples_per_user = max(1, (sample_size - n_anomalies) // len(unique_users))\n    normal_sampled = []\n    for user in unique_users:\n        user_normal = np.where((user_ids == user) & (is_anomaly == 0))[0]\n        if len(user_normal) > samples_per_user:\n            sampled = np.random.choice(user_normal, samples_per_user, replace=False)\n        else:\n            sampled = user_normal\n        normal_sampled.extend(sampled)\n    \n    # Adjust normal samples\n    if len(normal_sampled) > n_normal:\n        normal_sampled = np.random.choice(normal_sampled, n_normal, replace=False)\n    elif len(normal_sampled) < n_normal:\n        extra = np.random.choice(normal_indices, n_normal - len(normal_sampled), replace=False)\n        normal_sampled.extend(extra)\n    \n    # Sample anomalies\n    anomaly_sampled = np.random.choice(anomaly_indices, n_anomalies, replace=False) if len(anomaly_indices) >= n_anomalies else anomaly_indices\n    \n    indices = np.concatenate([normal_sampled, anomaly_sampled])\n    np.random.shuffle(indices)\n    indices = np.sort(indices)\n    \n    user_anomaly_rates = {}\n    for user in unique_users:\n        user_mask = user_ids == user\n        rate = np.mean(is_anomaly[user_mask]) if np.sum(user_mask) > 0 else 0.005\n        user_anomaly_rates[user] = min(rate, 0.05)\n    print(\"Computed user-specific anomaly rates (sample):\")\n    print({k: f\"{v:.3f}\" for k, v in list(user_anomaly_rates.items())[:5]})\n    \n    states = env.X[indices].astype(np.float32)\n    print(f\"Prepared {len(states)} states with shape {states.shape}, using all 19 features\")\n    true_labels = is_anomaly[indices]\n    sample_user_ids = user_ids[indices]\n    q_values = agent.act_batch(states)\n    anomaly_scores = q_values[:, 1]\n    \n    predictions = np.zeros_like(anomaly_scores, dtype=np.int32)\n    base_threshold = np.median(anomaly_scores)\n    for i, user_id in enumerate(sample_user_ids):\n        rate = user_anomaly_rates[user_id]\n        threshold = base_threshold * (1 - 0.1 * (rate / 0.005))\n        predictions[i] = 1 if anomaly_scores[i] > threshold else 0\n    \n    accuracy = accuracy_score(true_labels, predictions)\n    precision = precision_score(true_labels, predictions, zero_division=0)\n    recall = recall_score(true_labels, predictions, zero_division=0)\n    f1 = f1_score(true_labels, predictions, zero_division=0)\n    auc_roc = roc_auc_score(true_labels, anomaly_scores)\n    cm = confusion_matrix(true_labels, predictions)\n    tn, fp, fn, tp = cm.ravel()\n    \n    return {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1,\n        'auc_roc': auc_roc,\n        'false_negatives': fn,\n        'true_positives': tp,\n        'false_positives': fp,\n        'true_negatives': tn,\n        'confusion_matrix': cm.tolist()\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T14:31:48.317683Z","iopub.execute_input":"2025-04-16T14:31:48.317943Z","iopub.status.idle":"2025-04-16T14:31:48.328458Z","shell.execute_reply.started":"2025-04-16T14:31:48.317923Z","shell.execute_reply":"2025-04-16T14:31:48.327796Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validate data\nprint(f\"Overall anomaly rate: {np.mean(is_anomaly):.4f}\")\nunique_users = np.unique(user_ids)\noutlier_users = [u for u in unique_users if np.mean(is_anomaly[user_ids == u]) > 0.1]\nprint(f\"Users with anomaly rate >10%: {len(outlier_users)}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T10:40:54.139089Z","iopub.execute_input":"2025-04-16T10:40:54.139641Z","iopub.status.idle":"2025-04-16T10:41:16.030245Z","shell.execute_reply.started":"2025-04-16T10:40:54.139617Z","shell.execute_reply":"2025-04-16T10:41:16.029573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train\nprint(\"Training Fast DQN...\")\nagent = train_fast_dqn(X_combined, user_ids, is_anomaly, steps=10000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T14:32:38.455472Z","iopub.execute_input":"2025-04-16T14:32:38.456161Z","iopub.status.idle":"2025-04-16T16:15:41.463577Z","shell.execute_reply.started":"2025-04-16T14:32:38.456137Z","shell.execute_reply":"2025-04-16T16:15:41.462916Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate\nprint(\"\\nEvaluating Retrained Agent...\")\nenv = FullFeatureAnomalyEnv(X_combined, user_ids, is_anomaly)\nmetrics = evaluate_dqn_agent(env, agent, is_anomaly, user_ids, sample_size=10000)\nprint(\"\\nEvaluation Metrics (User-Aware, Stratified Sample):\")\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T16:16:23.430446Z","iopub.execute_input":"2025-04-16T16:16:23.430939Z","iopub.status.idle":"2025-04-16T16:17:08.059457Z","shell.execute_reply.started":"2025-04-16T16:16:23.430917Z","shell.execute_reply":"2025-04-16T16:17:08.058683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"role based + user based adaptive thresholding","metadata":{}},{"cell_type":"code","source":"# Hyperparameters\nSTATE_DIM = 19 \nROLE_COL_IDX = 10  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:58:27.750571Z","iopub.execute_input":"2025-04-16T17:58:27.751355Z","iopub.status.idle":"2025-04-16T17:58:27.754745Z","shell.execute_reply.started":"2025-04-16T17:58:27.751329Z","shell.execute_reply":"2025-04-16T17:58:27.754094Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FullFeatureAnomalyEnv:\n    def __init__(self, X_combined, user_ids, labels):\n        \"\"\"\n        Environment using all 19 features:\n        - Indices 0-9: Statistical (manual) features\n        - Index 10: Role\n        - Indices 11-18: LSTM autoencoder latent vectors\n        \"\"\"\n        if X_combined.shape[1] != 19:\n            raise ValueError(f\"Expected 19 features, got {X_combined.shape[1]}\")\n        self.X = X_combined\n        self.user_ids = user_ids\n        self.labels = labels\n        self.current_idx = 0\n        self.state_dim = 19\n        self.action_space = 2\n        print(\"Initialized environment with state_dim=19, including:\")\n        print(\"- First 10 statistical features (indices 0-9)\")\n        print(\"- Role feature (index 10)\")\n        print(\"- 8 latent vectors (indices 11-18)\")\n\n    def reset(self):\n        self.current_idx = random.randint(0, len(self.X)-1)\n        return self._get_state()\n\n    def _get_state(self):\n        return self.X[self.current_idx].astype(np.float32)\n\n    def step(self, action):\n        true_label = self.labels[self.current_idx]\n        if action == 1:\n            reward = 40.0 if true_label == 1 else -10.0\n        else:\n            reward = -20.0 if true_label == 1 else 30.0\n        self.current_idx = (self.current_idx + 1) % len(self.X)\n        done = False\n\n        return self._get_state(), reward, done, self.X[self.current_idx][10]  # Return role instead of user_id","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:58:45.350494Z","iopub.execute_input":"2025-04-16T17:58:45.350793Z","iopub.status.idle":"2025-04-16T17:58:45.359697Z","shell.execute_reply.started":"2025-04-16T17:58:45.350772Z","shell.execute_reply":"2025-04-16T17:58:45.358927Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FastDQNAgent:\n    def __init__(self, state_dim, action_dim, roles, labels, model_path=None):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.memory = deque(maxlen=5000)\n        self.role_priorities = deque(maxlen=5000)\n        self.gamma = 0.9\n        self.epsilon = 1.0 if not model_path else 0.0\n        self.epsilon_min = 0.01\n        self.epsilon_decay = 0.999\n        self.batch_size = 128\n        \n        # Role-based anomaly rates\n        self.role_anomaly_rates = {}\n        unique_roles = np.unique(roles)\n        for role in unique_roles:\n            role_mask = roles == role\n            rate = np.mean(labels[role_mask]) if np.sum(role_mask) > 0 else 0.005\n            self.role_anomaly_rates[role] = min(rate, 0.05)  # Cap at 5%\n        \n        if model_path:\n            self.model = tf.keras.models.load_model(\n                model_path,\n                custom_objects={'mse': tf.keras.losses.MeanSquaredError()}\n            )\n        else:\n            self.model = self._build_model()\n        print(f\"Model initialized with input shape (None, {state_dim})\")\n\n    def _build_model(self):\n        model = tf.keras.Sequential([\n            tf.keras.layers.Dense(32, activation='relu', input_dim=self.state_dim),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(16, activation='relu'),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(self.action_dim)\n        ])\n        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(0.001))\n        return model\n\n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.action_dim)\n        q_values = self.model.predict(state[np.newaxis], verbose=0)[0]\n        return np.argmax(q_values)\n\n    def act_batch(self, states):\n        q_values = self.model.predict(states, verbose=0)\n        return q_values\n\n    def remember(self, state, action, reward, next_state, done, role):\n        rate = self.role_anomaly_rates[role] / 0.005\n        priority = max(1e-2, abs(reward) * rate)\n        self.memory.append((state, action, reward, next_state, done))\n        self.role_priorities.append(priority)\n\n    def replay(self):\n        if len(self.memory) < self.batch_size:\n            return\n        priorities = np.array(self.role_priorities)\n        priorities = np.clip(priorities, 1e-2, None)\n        priority_sum = max(priorities.sum(), 1e-6)\n        probs = priorities / priority_sum\n        try:\n            indices = np.random.choice(len(self.memory), self.batch_size, p=probs)\n        except ValueError:\n            print(\"Warning: Invalid probabilities, using uniform sampling\")\n            indices = np.random.choice(len(self.memory), self.batch_size)\n        minibatch = [self.memory[i] for i in indices]\n        \n        states = np.array([x[0] for x in minibatch])\n        actions = np.array([x[1] for x in minibatch])\n        rewards = np.array([x[2] for x in minibatch])\n        next_states = np.array([x[3] for x in minibatch])\n        \n        targets = rewards + self.gamma * np.amax(self.model.predict(next_states, verbose=0), axis=1)\n        target_vec = self.model.predict(states, verbose=0)\n        target_vec[np.arange(self.batch_size), actions] = targets\n        \n        self.model.train_on_batch(states, target_vec)\n\n    def get_threshold(self, scores, role):\n        \"\"\"Role-based adaptive threshold\"\"\"\n        base = np.percentile(scores, 80)  # Fixed percentile\n        role_rate = min(self.role_anomaly_rates[role], 0.02)  # Cap at 2%\n        adjustment = 0.03 * role_rate  # Simplified adjustment\n        return max(0.5 * base, base * (1 - adjustment))\n\n    def predict(self, X):\n        \"\"\"Predict anomalies with role-based thresholding\"\"\"\n        scores = self.model.predict(X, verbose=0)[:, 1]  # Q-value for anomaly\n        predictions = np.zeros(len(X), dtype=int)\n        roles = X[:, 10]  # Role feature\n        for i, (score, role) in enumerate(zip(scores, roles)):\n            threshold = self.get_threshold(scores, role)\n            predictions[i] = 1 if score > threshold else 0\n        return predictions, scores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T17:59:14.795831Z","iopub.execute_input":"2025-04-16T17:59:14.796362Z","iopub.status.idle":"2025-04-16T17:59:14.80968Z","shell.execute_reply.started":"2025-04-16T17:59:14.796333Z","shell.execute_reply":"2025-04-16T17:59:14.809023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent = train_fast_dqn(X_combined, user_ids, is_anomaly, steps=8000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T18:00:41.451144Z","iopub.execute_input":"2025-04-16T18:00:41.45207Z","iopub.status.idle":"2025-04-16T19:14:41.956041Z","shell.execute_reply.started":"2025-04-16T18:00:41.452027Z","shell.execute_reply":"2025-04-16T19:14:41.955359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_role_aware(agent, X_test, y_test):\n    \"\"\"\n    Evaluates model with role-specific metrics\n    Returns: Overall metrics + per-role breakdown\n    \"\"\"\n    # Get predictions and scores\n    preds, scores = agent.predict(X_test)\n    roles = X_test[:, 10]  # Role feature column\n    \n    # Overall metrics\n    metrics = {\n        'accuracy': accuracy_score(y_test, preds),\n        'precision': precision_score(y_test, preds, zero_division=0),\n        'recall': recall_score(y_test, preds, zero_division=0),\n        'f1': f1_score(y_test, preds, zero_division=0),\n        'auc_roc': roc_auc_score(y_test, scores),\n        'confusion_matrix': confusion_matrix(y_test, preds),\n    }\n    \n    # Per-role metrics\n    role_metrics = {}\n    for role in np.unique(roles):\n        mask = (roles == role)\n        if sum(mask) > 0:  # Avoid division by zero\n            role_metrics[role] = {\n                'precision': precision_score(y_test[mask], preds[mask], zero_division=0),\n                'recall': recall_score(y_test[mask], preds[mask], zero_division=0),\n                'f1': f1_score(y_test[mask], preds[mask], zero_division=0),\n                'support': sum(mask),\n                'threshold': agent.get_threshold(scores[mask], role)  # Show actual threshold used\n            }\n    \n    return {'overall': metrics, 'role_wise': role_metrics}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:57:25.089907Z","iopub.execute_input":"2025-04-16T19:57:25.090729Z","iopub.status.idle":"2025-04-16T19:57:25.097513Z","shell.execute_reply.started":"2025-04-16T19:57:25.090703Z","shell.execute_reply":"2025-04-16T19:57:25.096795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_metrics = evaluate_role_aware(agent, X_combined, is_anomaly)\nprint(\"Overall Metrics:\", test_metrics['overall'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:57:28.716963Z","iopub.execute_input":"2025-04-16T19:57:28.717604Z","iopub.status.idle":"2025-04-16T20:22:18.104062Z","shell.execute_reply.started":"2025-04-16T19:57:28.717578Z","shell.execute_reply":"2025-04-16T20:22:18.103347Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_thresholds(agent, X_test):\n    \"\"\"Analyzes threshold distribution across roles\"\"\"\n    scores = agent.model.predict(X_test, verbose=0)[:, 1]\n    roles = X_test[:, 10]\n    \n    threshold_data = []\n    for role in np.unique(roles):\n        role_scores = scores[roles == role]\n        if len(role_scores) > 0:\n            threshold = agent.get_threshold(role_scores, role)\n            threshold_data.append({\n                'role': role,\n                'avg_score': np.mean(role_scores),\n                'threshold': threshold,\n                'anomaly_rate': agent.role_anomaly_rates.get(role, 0)\n            })\n    \n    return pd.DataFrame(threshold_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:23:22.408244Z","iopub.execute_input":"2025-04-16T19:23:22.409116Z","iopub.status.idle":"2025-04-16T19:23:22.414574Z","shell.execute_reply.started":"2025-04-16T19:23:22.409071Z","shell.execute_reply":"2025-04-16T19:23:22.413742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_combined = X_combined.astype(np.float32)  # Convert ALL features to float32\nis_anomaly = is_anomaly.astype(np.int32)    # Labels as 0/1 integers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:23:55.61567Z","iopub.execute_input":"2025-04-16T19:23:55.615957Z","iopub.status.idle":"2025-04-16T19:23:56.036565Z","shell.execute_reply.started":"2025-04-16T19:23:55.615935Z","shell.execute_reply":"2025-04-16T19:23:56.035841Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold_df = analyze_thresholds(agent, X_combined)\nprint(threshold_df.sort_values('threshold'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T19:23:58.497351Z","iopub.execute_input":"2025-04-16T19:23:58.49788Z","iopub.status.idle":"2025-04-16T19:24:15.972781Z","shell.execute_reply.started":"2025-04-16T19:23:58.497853Z","shell.execute_reply":"2025-04-16T19:24:15.972091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"baseline models","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import IsolationForest\nfrom sklearn.svm import OneClassSVM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:37:48.527637Z","iopub.execute_input":"2025-04-16T20:37:48.528297Z","iopub.status.idle":"2025-04-16T20:37:48.874204Z","shell.execute_reply.started":"2025-04-16T20:37:48.528259Z","shell.execute_reply":"2025-04-16T20:37:48.873444Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nX_train, X_test, y_train, y_test= train_test_split(\n        X_combined, is_anomaly, test_size=0.2, stratify=is_anomaly\n    )\n    \n    # 1. Isolation Forest\niso = IsolationForest(contamination=0.004, random_state=42)\niso_preds = iso.fit_predict(X_train)\niso_preds = [1 if x == -1 else 0 for x in iso_preds]\n    \n# 2. One-Class SVM\nsvm = OneClassSVM(nu=0.004)\nsvm.fit(X_train[y_train == 0])  # Train only on normals\nsvm_preds = svm.predict(X_train)\nsvm_preds = [1 if x == -1 else 0 for x in svm_preds]\n    \n\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:38:28.592367Z","iopub.execute_input":"2025-04-16T20:38:28.59294Z","iopub.status.idle":"2025-04-16T20:48:41.754788Z","shell.execute_reply.started":"2025-04-16T20:38:28.592916Z","shell.execute_reply":"2025-04-16T20:48:41.75379Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import classification_report\nprint(\"\\nIsolation Forest:\")\nprint(classification_report(y_train, iso_preds, target_names=[\"Normal\", \"Anomaly\"]))\n    \nprint(\"\\nOne-Class SVM:\")\nprint(classification_report(y_train, svm_preds, target_names=[\"Normal\", \"Anomaly\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-16T20:54:04.08787Z","iopub.execute_input":"2025-04-16T20:54:04.088616Z","iopub.status.idle":"2025-04-16T20:54:05.31034Z","shell.execute_reply.started":"2025-04-16T20:54:04.088588Z","shell.execute_reply":"2025-04-16T20:54:05.30957Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}